{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bb39c9a",
   "metadata": {},
   "source": [
    "# Enhanced GCN-LSTM with Static Node Features for Crime Prediction\n",
    "\n",
    "This notebook extends the original GCN-LSTM architecture by incorporating static socioeconomic and demographic features through a sophisticated attention mechanism. The static features are integrated as node-level attributes that modulate temporal crime predictions.\n",
    "\n",
    "## Key Enhancements:\n",
    "1. **Static Feature Integration**: External socioeconomic features from CSV files\n",
    "2. **Cross-Modal Attention**: Separate attention mechanism for static-temporal interaction\n",
    "3. **Multi-Head Attention**: Multiple attention heads for different feature aspects\n",
    "4. **Feature-Aware Graph Convolution**: Static features influence spatial relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7757025",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8e4587f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "PyTorch version: 2.5.1\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import copy\n",
    "import warnings\n",
    "import zipfile\n",
    "import io\n",
    "import requests\n",
    "import pickle\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "# Configuration\n",
    "SEED = 42\n",
    "WINDOW_SIZE = 3\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dd3559",
   "metadata": {},
   "source": [
    "## 2. Data Download and Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "174745a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_crime_data(data_dir=None, force_download=False):\n",
    "    \"\"\"Download crime data and shapefiles\"\"\"\n",
    "    urls = {\n",
    "        'recent_crime': 'https://raw.githubusercontent.com/IflyNY2PR/DSSS_cw/6bac9ee3834c73d705106153bf91b315bb1faf01/MPS%20LSOA%20Level%20Crime%20(most%20recent%2024%20months).csv',\n",
    "        'historical_crime': 'https://raw.githubusercontent.com/IflyNY2PR/DSSS_cw/refs/heads/main/MPS%20LSOA%20Level%20Crime%20(Historical).csv',\n",
    "        'shapefile': 'https://github.com/IflyNY2PR/DSSS_cw/raw/main/statistical-gis-boundaries-london.zip'\n",
    "    }\n",
    "    \n",
    "    data_dir = Path('./crime_data') if data_dir is None else Path(data_dir)\n",
    "    data_dir.mkdir(exist_ok=True)\n",
    "    shapefile_dir = data_dir / 'shapefiles'\n",
    "    shapefile_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    paths = {\n",
    "        'recent_crime': str(data_dir / 'recent_crime.csv'),\n",
    "        'historical_crime': str(data_dir / 'historical_crime.csv')\n",
    "    }\n",
    "    \n",
    "    files_exist = all([\n",
    "        Path(paths['recent_crime']).exists(),\n",
    "        Path(paths['historical_crime']).exists(),\n",
    "        (shapefile_dir / 'statistical-gis-boundaries-london').exists()\n",
    "    ])\n",
    "    \n",
    "    if not files_exist or force_download:\n",
    "        print(\"Downloading crime data files...\")\n",
    "        for name in ['recent_crime', 'historical_crime']:\n",
    "            print(f\"Downloading {name}...\")\n",
    "            pd.read_csv(urls[name]).to_csv(paths[name], index=False)\n",
    "        \n",
    "        print(\"Downloading and extracting shapefile...\")\n",
    "        try:\n",
    "            r = requests.get(urls['shapefile'])\n",
    "            r.raise_for_status()\n",
    "            z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "            z.extractall(shapefile_dir)\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading shapefile: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"Crime data files already exist.\")\n",
    "    \n",
    "    return paths\n",
    "\n",
    "def download_static_features(data_dir=None, force_download=False):\n",
    "    \"\"\"Download static feature matrices\"\"\"\n",
    "    base_url = \"https://raw.githubusercontent.com/IflyNY2PR/CASA0004/013cb9ac54cc8a890e06437567ef4f6dff140ee7/data-preparation/\"\n",
    "    \n",
    "    files = {\n",
    "        'documentation': 'gcn_feature_matrix_documentation.csv',\n",
    "        'spatial_imputed': 'gcn_feature_matrix_spatial_imputed.csv',\n",
    "        'spatial_imputed_scaled': 'gcn_feature_matrix_spatial_imputed_scaled.csv',\n",
    "        'summary_stats': 'gcn_feature_matrix_summary_stats.csv',\n",
    "        'with_geometry': 'gcn_feature_matrix_with_geometry.csv'\n",
    "    }\n",
    "    \n",
    "    data_dir = Path('./static_features') if data_dir is None else Path(data_dir)\n",
    "    data_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    paths = {}\n",
    "    for key, filename in files.items():\n",
    "        filepath = data_dir / filename\n",
    "        paths[key] = str(filepath)\n",
    "        \n",
    "        if not filepath.exists() or force_download:\n",
    "            print(f\"Downloading {filename}...\")\n",
    "            try:\n",
    "                df = pd.read_csv(base_url + filename)\n",
    "                df.to_csv(filepath, index=False)\n",
    "                print(f\"Successfully downloaded {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading {filename}: {e}\")\n",
    "    \n",
    "    return paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6009b2f9",
   "metadata": {},
   "source": [
    "## 3. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd169d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crime data files already exist.\n",
      "\n",
      "Loading crime data...\n",
      "\n",
      "Loading static features...\n",
      "\n",
      "Recent crime data shape: (100868, 29)\n",
      "Historical crime data shape: (113116, 161)\n",
      "Static features (scaled) shape: (4719, 17)\n",
      "London GeoDataFrame shape: (4835, 15)\n",
      "\n",
      "Loading static features...\n",
      "\n",
      "Recent crime data shape: (100868, 29)\n",
      "Historical crime data shape: (113116, 161)\n",
      "Static features (scaled) shape: (4719, 17)\n",
      "London GeoDataFrame shape: (4835, 15)\n"
     ]
    }
   ],
   "source": [
    "# Download all data\n",
    "crime_paths = download_crime_data()\n",
    "static_paths = download_static_features()\n",
    "\n",
    "# Load crime data\n",
    "print(\"\\nLoading crime data...\")\n",
    "recent_crime_df = pd.read_csv(crime_paths['recent_crime'])\n",
    "historical_crime_df = pd.read_csv(crime_paths['historical_crime'])\n",
    "\n",
    "# Load static features\n",
    "print(\"\\nLoading static features...\")\n",
    "static_features_scaled = pd.read_csv(static_paths['spatial_imputed_scaled'])\n",
    "static_features_raw = pd.read_csv(static_paths['spatial_imputed'])\n",
    "feature_documentation = pd.read_csv(static_paths['documentation'])\n",
    "summary_stats = pd.read_csv(static_paths['summary_stats'])\n",
    "\n",
    "# Load shapefiles\n",
    "shapefile_path = Path('./crime_data/shapefiles/statistical-gis-boundaries-london/ESRI/LSOA_2011_London_gen_MHW.shp')\n",
    "london_gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "print(f\"\\nRecent crime data shape: {recent_crime_df.shape}\")\n",
    "print(f\"Historical crime data shape: {historical_crime_df.shape}\")\n",
    "print(f\"Static features (scaled) shape: {static_features_scaled.shape}\")\n",
    "print(f\"London GeoDataFrame shape: {london_gdf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832c1fd9",
   "metadata": {},
   "source": [
    "## 4. Explore Static Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81b54429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Documentation:\n",
      "                   Feature                                       Description  \\\n",
      "0                 AvgPrice             Average housing price (March 2023, £)   \n",
      "1                 MeanPTAL         Mean Public Transport Accessibility Index   \n",
      "2               MedianPTAL       Median Public Transport Accessibility Index   \n",
      "3               Population                          Population count in LSOA   \n",
      "4                 Area_km2  LSOA area (square kilometers, from PTAL dataset)   \n",
      "5            MeanSentiment                 Average sentiment score (-1 to 1)   \n",
      "6              SentimentSD            Standard deviation of sentiment scores   \n",
      "7              ReviewCount                  Number of reviews/posts analyzed   \n",
      "8         NearestStation_m              Distance to nearest station (meters)   \n",
      "9       StationsWithin500m                    Number of stations within 500m   \n",
      "10           NearestRail_m            Distance to nearest rail line (meters)   \n",
      "11          StreetLength_m                      Total street length (meters)   \n",
      "12  StreetDensity_m_per_m2                             Street density (m/m²)   \n",
      "13          StreetSegments                         Number of street segments   \n",
      "14       LandUse_Diversity                          Number of land use types   \n",
      "15            LandUse_Area                          Total land use area (m²)   \n",
      "\n",
      "                 Data_Type     Missing_Values         Standardized  \n",
      "0   float64 (standardized)  Spatially imputed  Yes (mean≈0, std≈1)  \n",
      "1   float64 (standardized)  Spatially imputed  Yes (mean≈0, std≈1)  \n",
      "2   float64 (standardized)  Spatially imputed  Yes (mean≈0, std≈1)  \n",
      "3   float64 (standardized)  Spatially imputed  Yes (mean≈0, std≈1)  \n",
      "4   float64 (standardized)  Spatially imputed  Yes (mean≈0, std≈1)  \n",
      "5   float64 (standardized)  Spatially imputed  Yes (mean≈0, std≈1)  \n",
      "6   float64 (standardized)  Spatially imputed  Yes (mean≈0, std≈1)  \n",
      "7   float64 (standardized)  Spatially imputed  Yes (mean≈0, std≈1)  \n",
      "8   float64 (standardized)  Spatially imputed  Yes (mean≈0, std≈1)  \n",
      "9   float64 (standardized)  Spatially imputed  Yes (mean≈0, std≈1)  \n",
      "10  float64 (standardized)  Spatially imputed  Yes (mean≈0, std≈1)  \n",
      "11  float64 (standardized)  Spatially imputed  Yes (mean≈0, std≈1)  \n",
      "12  float64 (standardized)  Spatially imputed  Yes (mean≈0, std≈1)  \n",
      "13  float64 (standardized)  Spatially imputed  Yes (mean≈0, std≈1)  \n",
      "14  float64 (standardized)  Spatially imputed  Yes (mean≈0, std≈1)  \n",
      "15  float64 (standardized)  Spatially imputed  Yes (mean≈0, std≈1)  \n",
      "\n",
      "Total number of static features: 16\n",
      "\n",
      "Summary Statistics Sample:\n",
      "  Unnamed: 0      AvgPrice      MeanPTAL    MedianPTAL    Population  \\\n",
      "0      count  4.719000e+03  4.719000e+03  4.719000e+03  4.719000e+03   \n",
      "1       mean  3.011412e-17  3.613695e-17  4.818260e-17  5.149515e-16   \n",
      "2        std  1.000106e+00  1.000106e+00  1.000106e+00  1.000106e+00   \n",
      "3        min -1.152211e+00 -1.135806e+00 -1.141877e+00 -6.525909e+00   \n",
      "4        25% -5.124578e-01 -6.406866e-01 -6.386678e-01 -2.499702e-01   \n",
      "\n",
      "       Area_km2  MeanSentiment   SentimentSD   ReviewCount  NearestStation_m  \\\n",
      "0  4.719000e+03   4.719000e+03  4.719000e+03  4.719000e+03      4.719000e+03   \n",
      "1 -3.613695e-17   7.325260e-16  3.312554e-17  6.022825e-18      2.107989e-16   \n",
      "2  1.000106e+00   1.000106e+00  1.000106e+00  1.000106e+00      1.000106e+00   \n",
      "3 -6.939729e-01  -4.580052e+00 -1.620051e+00 -1.541954e-01     -1.551074e+00   \n",
      "4 -3.958224e-01  -3.423803e-01 -7.086264e-01 -1.502329e-01     -7.001013e-01   \n",
      "\n",
      "   StationsWithin500m  NearestRail_m  StreetLength_m  StreetDensity_m_per_m2  \\\n",
      "0        4.719000e+03   4.719000e+03    4.719000e+03            4.719000e+03   \n",
      "1       -6.022825e-17  -4.818260e-17    1.806847e-16            6.022825e-17   \n",
      "2        1.000106e+00   1.000106e+00    1.000106e+00            1.000106e+00   \n",
      "3       -2.009234e+00  -1.243591e+00   -1.919367e+00           -2.527152e+00   \n",
      "4       -7.511745e-01  -6.965832e-01   -5.772423e-01           -6.859315e-01   \n",
      "\n",
      "   StreetSegments  LandUse_Diversity  LandUse_Area  \n",
      "0    4.719000e+03       4.719000e+03  4.719000e+03  \n",
      "1    7.528531e-18       1.445478e-16  4.215977e-17  \n",
      "2    1.000106e+00       1.000106e+00  1.000106e+00  \n",
      "3   -2.034934e+00      -1.983594e+00 -6.596514e-01  \n",
      "4   -5.966231e-01      -8.417118e-01 -3.708117e-01  \n",
      "Available LSOA columns in static features: ['LSOA_CODE']\n",
      "Using static LSOA column: LSOA_CODE\n",
      "\n",
      "LSOAs in crime data: 4988\n",
      "LSOAs in static features: 4719\n",
      "Common LSOAs: 4541\n",
      "Coverage: 91.0%\n"
     ]
    }
   ],
   "source": [
    "# Display feature documentation\n",
    "print(\"Feature Documentation:\")\n",
    "print(feature_documentation.head(20))\n",
    "print(f\"\\nTotal number of static features: {len(feature_documentation)}\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nSummary Statistics Sample:\")\n",
    "print(summary_stats.head())\n",
    "\n",
    "# Check LSOA alignment\n",
    "crime_lsoas = set(recent_crime_df['LSOA Code'].unique())\n",
    "\n",
    "# Find the correct LSOA column name in static features\n",
    "lsoa_cols = [col for col in static_features_scaled.columns if 'LSOA' in col.upper()]\n",
    "print(f\"Available LSOA columns in static features: {lsoa_cols}\")\n",
    "\n",
    "# Use the first LSOA column found, or default to common alternatives\n",
    "if lsoa_cols:\n",
    "    static_lsoa_col = lsoa_cols[0]\n",
    "elif 'LSOA_CODE' in static_features_scaled.columns:\n",
    "    static_lsoa_col = 'LSOA_CODE'\n",
    "else:\n",
    "    # Print all columns to help debug\n",
    "    print(f\"Available columns in static features: {list(static_features_scaled.columns)}\")\n",
    "    raise ValueError(\"Could not find LSOA column in static features\")\n",
    "\n",
    "print(f\"Using static LSOA column: {static_lsoa_col}\")\n",
    "static_lsoas = set(static_features_scaled[static_lsoa_col].unique())\n",
    "common_lsoas = crime_lsoas.intersection(static_lsoas)\n",
    "\n",
    "print(f\"\\nLSOAs in crime data: {len(crime_lsoas)}\")\n",
    "print(f\"LSOAs in static features: {len(static_lsoas)}\")\n",
    "print(f\"Common LSOAs: {len(common_lsoas)}\")\n",
    "print(f\"Coverage: {len(common_lsoas)/len(crime_lsoas)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c15023b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Static features columns:\n",
      "0: LSOA_CODE\n",
      "1: AvgPrice\n",
      "2: MeanPTAL\n",
      "3: MedianPTAL\n",
      "4: Population\n",
      "5: Area_km2\n",
      "6: MeanSentiment\n",
      "7: SentimentSD\n",
      "8: ReviewCount\n",
      "9: NearestStation_m\n",
      "10: StationsWithin500m\n",
      "11: NearestRail_m\n",
      "12: StreetLength_m\n",
      "13: StreetDensity_m_per_m2\n",
      "14: StreetSegments\n",
      "15: LandUse_Diversity\n",
      "16: LandUse_Area\n",
      "\n",
      "Total columns: 17\n",
      "Shape: (4719, 17)\n",
      "\n",
      "LSOA columns found: ['LSOA_CODE']\n",
      "ID-like columns found: ['LSOA_CODE']\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check column names in static features\n",
    "print(\"Static features columns:\")\n",
    "for i, col in enumerate(static_features_scaled.columns):\n",
    "    print(f\"{i}: {col}\")\n",
    "    \n",
    "print(f\"\\nTotal columns: {len(static_features_scaled.columns)}\")\n",
    "print(f\"Shape: {static_features_scaled.shape}\")\n",
    "\n",
    "# Check for LSOA columns specifically\n",
    "lsoa_cols = [col for col in static_features_scaled.columns if 'LSOA' in col.upper()]\n",
    "print(f\"\\nLSOA columns found: {lsoa_cols}\")\n",
    "\n",
    "# Check for alternative ID columns\n",
    "id_cols = [col for col in static_features_scaled.columns if any(x in col.upper() for x in ['CODE', 'ID', 'CD'])]\n",
    "print(f\"ID-like columns found: {id_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20e72af",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e7528f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined crime dataset shape: (20066928, 10)\n",
      "Date range: 2010-04-01 00:00:00 to 2025-03-01 00:00:00\n",
      "Filtered crime dataset shape: (18266172, 10)\n",
      "Filtered crime dataset shape: (18266172, 10)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_crime_data(historical_df, recent_df):\n",
    "    \"\"\"Combine and preprocess crime data\"\"\"\n",
    "    non_date_columns = ['LSOA Code', 'LSOA Name', 'Borough', 'Major Category', 'Minor Category']\n",
    "    \n",
    "    historical_date_cols = [col for col in historical_df.columns if col not in non_date_columns]\n",
    "    recent_date_cols = [col for col in recent_df.columns if col not in non_date_columns]\n",
    "    \n",
    "    historical_melted = pd.melt(\n",
    "        historical_df,\n",
    "        id_vars=non_date_columns,\n",
    "        value_vars=historical_date_cols,\n",
    "        var_name='date',\n",
    "        value_name='count'\n",
    "    )\n",
    "    \n",
    "    recent_melted = pd.melt(\n",
    "        recent_df,\n",
    "        id_vars=non_date_columns,\n",
    "        value_vars=recent_date_cols,\n",
    "        var_name='date',\n",
    "        value_name='count'\n",
    "    )\n",
    "    \n",
    "    combined_df = pd.concat([historical_melted, recent_melted])\n",
    "    combined_df['date'] = pd.to_datetime(combined_df['date'] + '01', format='%Y%m%d')\n",
    "    \n",
    "    # Remove duplicates\n",
    "    combined_df = combined_df.drop_duplicates(\n",
    "        subset=['LSOA Code', 'Major Category', 'Minor Category', 'date'],\n",
    "        keep='last'\n",
    "    )\n",
    "    \n",
    "    combined_df = combined_df.sort_values(['date', 'LSOA Code', 'Major Category', 'Minor Category'])\n",
    "    \n",
    "    # Add temporal features\n",
    "    combined_df['month'] = combined_df['date'].dt.month\n",
    "    combined_df['year'] = combined_df['date'].dt.year\n",
    "    combined_df['day_of_week'] = combined_df['date'].dt.dayofweek\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Preprocess crime data\n",
    "crime_df = preprocess_crime_data(historical_crime_df, recent_crime_df)\n",
    "print(f\"Combined crime dataset shape: {crime_df.shape}\")\n",
    "print(f\"Date range: {crime_df['date'].min()} to {crime_df['date'].max()}\")\n",
    "\n",
    "# Filter to common LSOAs\n",
    "crime_df = crime_df[crime_df['LSOA Code'].isin(common_lsoas)]\n",
    "print(f\"Filtered crime dataset shape: {crime_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff494dec",
   "metadata": {},
   "source": [
    "## 6. Create Adjacency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06502156",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating adjacency matrix: 100%|██████████| 4541/4541 [00:02<00:00, 1950.44it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency matrix shape: (4541, 4541)\n"
     ]
    }
   ],
   "source": [
    "def create_adjacency_matrix(gdf, region_id_col='LSOA11CD', regions=None):\n",
    "    \"\"\"Create adjacency matrix from shapefile\"\"\"\n",
    "    if regions is not None:\n",
    "        gdf = gdf[gdf[region_id_col].isin(regions)].copy()\n",
    "    \n",
    "    region_list = gdf[region_id_col].tolist()\n",
    "    n_regions = len(region_list)\n",
    "    region_to_idx = {region: i for i, region in enumerate(region_list)}\n",
    "    \n",
    "    adj_matrix = np.zeros((n_regions, n_regions))\n",
    "    \n",
    "    for i, region in enumerate(tqdm(region_list, desc=\"Creating adjacency matrix\")):\n",
    "        geom = gdf.loc[gdf[region_id_col] == region, 'geometry'].iloc[0]\n",
    "        neighbors = gdf[gdf.geometry.touches(geom)][region_id_col].tolist()\n",
    "        \n",
    "        for neighbor in neighbors:\n",
    "            if neighbor in region_to_idx:\n",
    "                j = region_to_idx[neighbor]\n",
    "                adj_matrix[i, j] = 1\n",
    "                adj_matrix[j, i] = 1\n",
    "    \n",
    "    # Add self-loops\n",
    "    np.fill_diagonal(adj_matrix, 1)\n",
    "    \n",
    "    return adj_matrix, region_list\n",
    "\n",
    "# Create adjacency matrix\n",
    "adjacency_matrix, region_list = create_adjacency_matrix(\n",
    "    london_gdf, region_id_col='LSOA11CD', regions=list(common_lsoas)\n",
    ")\n",
    "print(f\"Adjacency matrix shape: {adjacency_matrix.shape}\")\n",
    "\n",
    "# Normalize adjacency matrix\n",
    "def normalize_adjacency(adj):\n",
    "    \"\"\"Normalize adjacency matrix with D^(-1/2) * A * D^(-1/2)\"\"\"\n",
    "    adj_with_self = adj + np.eye(adj.shape[0])\n",
    "    degrees = np.array(adj_with_self.sum(1))\n",
    "    D_inv_sqrt = np.diag(np.power(degrees, -0.5).flatten())\n",
    "    normalized_adj = D_inv_sqrt.dot(adj_with_self).dot(D_inv_sqrt)\n",
    "    return normalized_adj\n",
    "\n",
    "A_hat = normalize_adjacency(adjacency_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a53fe57",
   "metadata": {},
   "source": [
    "## 7. Prepare Static Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "994b2105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns in static_df: ['LSOA_CODE', 'AvgPrice', 'MeanPTAL', 'MedianPTAL', 'Population', 'Area_km2', 'MeanSentiment', 'SentimentSD', 'ReviewCount', 'NearestStation_m']...\n",
      "LSOA-related columns: ['LSOA_CODE']\n",
      "Using LSOA column: LSOA_CODE\n",
      "Static feature matrix shape: (4541, 16)\n",
      "Number of features: 16\n",
      "Sample feature names: ['AvgPrice', 'MeanPTAL', 'MedianPTAL', 'Population', 'Area_km2', 'MeanSentiment', 'SentimentSD', 'ReviewCount', 'NearestStation_m', 'StationsWithin500m']\n"
     ]
    }
   ],
   "source": [
    "def prepare_static_features(static_df, region_list):\n",
    "    \"\"\"Prepare static feature matrix aligned with region list\"\"\"\n",
    "    # Debug: Check what columns are available\n",
    "    print(f\"Available columns in static_df: {list(static_df.columns)[:10]}...\")\n",
    "    \n",
    "    # Find the LSOA column (it might have a different name)\n",
    "    lsoa_cols = [col for col in static_df.columns if 'LSOA' in col.upper()]\n",
    "    print(f\"LSOA-related columns: {lsoa_cols}\")\n",
    "    \n",
    "    # Use the correct LSOA column name\n",
    "    lsoa_col = lsoa_cols[0] if lsoa_cols else 'LSOA_CODE'  # fallback to common alternative\n",
    "    print(f\"Using LSOA column: {lsoa_col}\")\n",
    "    \n",
    "    # Ensure proper ordering\n",
    "    static_df = static_df.set_index(lsoa_col)\n",
    "    \n",
    "    # Select feature columns (exclude ID columns)\n",
    "    feature_cols = [col for col in static_df.columns if col not in [lsoa_col, 'geometry']]\n",
    "    \n",
    "    # Create feature matrix aligned with region_list\n",
    "    feature_matrix = np.zeros((len(region_list), len(feature_cols)))\n",
    "    \n",
    "    for i, region in enumerate(region_list):\n",
    "        if region in static_df.index:\n",
    "            feature_matrix[i] = static_df.loc[region, feature_cols].values\n",
    "        else:\n",
    "            # Use mean imputation for missing regions\n",
    "            feature_matrix[i] = static_df[feature_cols].mean().values\n",
    "    \n",
    "    return feature_matrix, feature_cols\n",
    "\n",
    "# Prepare static features\n",
    "static_feature_matrix, feature_names = prepare_static_features(static_features_scaled, region_list)\n",
    "print(f\"Static feature matrix shape: {static_feature_matrix.shape}\")\n",
    "print(f\"Number of features: {len(feature_names)}\")\n",
    "print(f\"Sample feature names: {feature_names[:10]}\")\n",
    "\n",
    "# Convert to tensor\n",
    "static_features_tensor = torch.FloatTensor(static_feature_matrix).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148f06a6",
   "metadata": {},
   "source": [
    "## 8. Enhanced Dataset with Static Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54563ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedCrimeDataset(Dataset):\n",
    "    def __init__(self, data, region_list, static_features, window_size, target_col, predict_ahead=1):\n",
    "        self.dates = sorted(data['date'].unique())\n",
    "        self.region_list = region_list\n",
    "        self.window_size = window_size\n",
    "        self.predict_ahead = predict_ahead\n",
    "        self.static_features = static_features  # [n_regions, n_features]\n",
    "        \n",
    "        # Pivot crime data\n",
    "        df_pivot = (\n",
    "            data\n",
    "            .pivot(index='date', columns='LSOA Code', values=target_col)\n",
    "            .reindex(index=self.dates, columns=self.region_list, fill_value=0)\n",
    "        )\n",
    "        self.crime_matrix = df_pivot.values\n",
    "        \n",
    "        # Create valid indices for sequences\n",
    "        L = len(self.dates) - window_size - predict_ahead + 1\n",
    "        self.indices = [(i, i + window_size + predict_ahead - 1) for i in range(L)]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        i, j = self.indices[idx]\n",
    "        \n",
    "        # Temporal crime features\n",
    "        X_crime = self.crime_matrix[i : i + self.window_size]\n",
    "        y = self.crime_matrix[j]\n",
    "        \n",
    "        # Static features (same for all time steps)\n",
    "        X_static = self.static_features\n",
    "        \n",
    "        return (torch.FloatTensor(X_crime), torch.FloatTensor(X_static)), torch.FloatTensor(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbae0d32",
   "metadata": {},
   "source": [
    "## 9. Enhanced Model Architecture with Static Feature Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ed30f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StaticFeatureAttention(nn.Module):\n",
    "    \"\"\"Multi-head attention mechanism for static feature integration\"\"\"\n",
    "    def __init__(self, static_dim, hidden_dim, num_heads=4, dropout=0.1):\n",
    "        super(StaticFeatureAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        \n",
    "        assert hidden_dim % num_heads == 0\n",
    "        \n",
    "        # Static feature projections\n",
    "        self.static_projection = nn.Linear(static_dim, hidden_dim)\n",
    "        \n",
    "        # Multi-head attention components\n",
    "        self.query_projection = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.key_projection = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value_projection = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Layer norm and dropout\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, temporal_features, static_features):\n",
    "        batch_size, num_nodes, hidden_dim = temporal_features.size()\n",
    "        \n",
    "        # Project static features\n",
    "        static_projected = self.static_projection(static_features)  # [batch_size, num_nodes, hidden_dim] or [num_nodes, hidden_dim]\n",
    "        \n",
    "        # Handle both batched and non-batched static features\n",
    "        if static_projected.dim() == 2:  # [num_nodes, hidden_dim]\n",
    "            static_projected = static_projected.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        # else: already has batch dimension [batch_size, num_nodes, hidden_dim]\n",
    "        \n",
    "        # Compute Q, K, V\n",
    "        Q = self.query_projection(temporal_features)\n",
    "        K = self.key_projection(static_projected)\n",
    "        V = self.value_projection(static_projected)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        Q = Q.view(batch_size, num_nodes, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, num_nodes, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, num_nodes, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        attention_output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        # Reshape and project\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous()\n",
    "        attention_output = attention_output.view(batch_size, num_nodes, hidden_dim)\n",
    "        output = self.output_projection(attention_output)\n",
    "        \n",
    "        # Residual connection and layer norm\n",
    "        output = self.layer_norm(temporal_features + self.dropout(output))\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "class EnhancedGraphConvLayer(nn.Module):\n",
    "    \"\"\"Graph convolution layer enhanced with static feature modulation\"\"\"\n",
    "    def __init__(self, in_features, out_features, static_dim, dropout=0.1):\n",
    "        super(EnhancedGraphConvLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Standard GCN weight\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        \n",
    "        # Static feature modulation\n",
    "        self.static_gate = nn.Sequential(\n",
    "            nn.Linear(static_dim, out_features),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        \n",
    "    def forward(self, input, adj, static_features):\n",
    "        # Standard graph convolution\n",
    "        input = self.dropout(input)\n",
    "        support = torch.matmul(input, self.weight)\n",
    "        output = torch.matmul(adj, support)\n",
    "        \n",
    "        # Static feature modulation\n",
    "        batch_size = input.size(0)\n",
    "        # static_features already has batch dimension [batch_size, num_nodes, static_dim]\n",
    "        # so we don't need to add another batch dimension\n",
    "        if static_features.dim() == 2:  # [num_nodes, static_dim]\n",
    "            static_gate = self.static_gate(static_features).unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        else:  # [batch_size, num_nodes, static_dim]\n",
    "            static_gate = self.static_gate(static_features)\n",
    "        output = output * static_gate\n",
    "        \n",
    "        return output\n",
    "\n",
    "class EnhancedGCN_LSTM(nn.Module):\n",
    "    \"\"\"GCN-LSTM with static feature attention mechanism\"\"\"\n",
    "    def __init__(self, window_size, num_nodes, static_dim, hidden_dim=64, \n",
    "                 lstm_hidden=128, out_dim=1, num_heads=4, dropout=0.1, lambda_mmd=0.1):\n",
    "        super(EnhancedGCN_LSTM, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.num_nodes = num_nodes\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm_hidden = lstm_hidden\n",
    "        self.lambda_mmd = lambda_mmd\n",
    "        \n",
    "        # Initial embeddings\n",
    "        self.crime_embedding = nn.Linear(1, hidden_dim)\n",
    "        self.static_embedding = nn.Linear(static_dim, hidden_dim)\n",
    "        \n",
    "        # Enhanced graph convolutions with static modulation\n",
    "        self.gc1 = EnhancedGraphConvLayer(hidden_dim, hidden_dim, static_dim, dropout)\n",
    "        self.gc2 = EnhancedGraphConvLayer(hidden_dim, hidden_dim, static_dim, dropout)\n",
    "        \n",
    "        # Static feature attention\n",
    "        self.static_attention = StaticFeatureAttention(static_dim, hidden_dim, num_heads, dropout)\n",
    "        \n",
    "        # Temporal modeling\n",
    "        self.lstm = nn.LSTM(hidden_dim, lstm_hidden, num_layers=2, \n",
    "                           batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # Temporal attention\n",
    "        self.temporal_attention = nn.MultiheadAttention(lstm_hidden, num_heads, dropout)\n",
    "        \n",
    "        # Output layers\n",
    "        self.fc1 = nn.Linear(lstm_hidden + hidden_dim, hidden_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim, out_dim)\n",
    "        \n",
    "        # Activation and regularization\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x_crime, x_static, adj):\n",
    "        batch_size, window_size, num_nodes = x_crime.size()\n",
    "        \n",
    "        # Process static features once\n",
    "        static_embedded = self.static_embedding(x_static)  # [num_nodes, hidden_dim]\n",
    "        \n",
    "        # Process temporal crime data\n",
    "        temporal_features = []\n",
    "        \n",
    "        for t in range(window_size):\n",
    "            # Get crime data at time t\n",
    "            x_t = x_crime[:, t, :].unsqueeze(-1)  # [batch, nodes, 1]\n",
    "            \n",
    "            # Initial embedding\n",
    "            h_t = self.crime_embedding(x_t)  # [batch, nodes, hidden_dim]\n",
    "            \n",
    "            # Graph convolutions with static modulation\n",
    "            h_t = self.gc1(h_t, adj, x_static)\n",
    "            h_t = self.relu(h_t)\n",
    "            h_t = self.gc2(h_t, adj, x_static)\n",
    "            h_t = self.relu(h_t)\n",
    "            \n",
    "            # Apply static feature attention\n",
    "            h_t, _ = self.static_attention(h_t, x_static)\n",
    "            \n",
    "            temporal_features.append(h_t)\n",
    "        \n",
    "        # Stack temporal features\n",
    "        temporal_stack = torch.stack(temporal_features, dim=1)  # [batch, window, nodes, hidden]\n",
    "        \n",
    "        # Process each node's temporal sequence with LSTM\n",
    "        lstm_outputs = []\n",
    "        for i in range(num_nodes):\n",
    "            node_sequence = temporal_stack[:, :, i, :]  # [batch, window, hidden]\n",
    "            lstm_out, _ = self.lstm(node_sequence)\n",
    "            lstm_outputs.append(lstm_out[:, -1, :])  # Take last output\n",
    "        \n",
    "        # Stack LSTM outputs\n",
    "        lstm_features = torch.stack(lstm_outputs, dim=1)  # [batch, nodes, lstm_hidden]\n",
    "        \n",
    "        # Apply temporal attention\n",
    "        lstm_features_transposed = lstm_features.transpose(0, 1)  # [nodes, batch, lstm_hidden]\n",
    "        attended_features, _ = self.temporal_attention(\n",
    "            lstm_features_transposed, \n",
    "            lstm_features_transposed, \n",
    "            lstm_features_transposed\n",
    "        )\n",
    "        attended_features = attended_features.transpose(0, 1)  # [batch, nodes, lstm_hidden]\n",
    "        \n",
    "        # Combine with static embeddings\n",
    "        static_expanded = static_embedded.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        combined = torch.cat([attended_features, static_expanded], dim=-1)\n",
    "        \n",
    "        # Final predictions\n",
    "        output = self.fc1(combined)\n",
    "        output = self.relu(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc_out(output).squeeze(-1)\n",
    "        \n",
    "        # Calculate MMD for regularization\n",
    "        initial_embedding = temporal_features[0]\n",
    "        final_embedding = temporal_features[-1]\n",
    "        mmd = self.maximum_mean_discrepancy(initial_embedding, final_embedding)\n",
    "        \n",
    "        return output, mmd\n",
    "    \n",
    "    def maximum_mean_discrepancy(self, x, y):\n",
    "        x = x.mean(dim=1)\n",
    "        y = y.mean(dim=1)\n",
    "        \n",
    "        def gaussian_kernel(a, b, sigma=1.0):\n",
    "            dist = torch.sum((a.unsqueeze(1) - b.unsqueeze(0)).pow(2), dim=2)\n",
    "            return torch.exp(-dist / (2 * sigma**2))\n",
    "        \n",
    "        xx = gaussian_kernel(x, x)\n",
    "        yy = gaussian_kernel(y, y)\n",
    "        xy = gaussian_kernel(x, y)\n",
    "        return torch.mean(xx) + torch.mean(yy) - 2 * torch.mean(xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e210fee",
   "metadata": {},
   "source": [
    "## 10. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e8f80e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_enhanced_model(model, train_loader, val_loader, adj, epochs, lr, patience, device):\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    model.to(device)\n",
    "    adj_tensor = torch.tensor(adj, dtype=torch.float32, device=device)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_state = None\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Training\n",
    "        model.train()\n",
    "        epoch_train_losses = []\n",
    "        \n",
    "        for (X_crime, X_static), y in train_loader:\n",
    "            X_crime, X_static, y = X_crime.to(device), X_static.to(device), y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            preds, mmd = model(X_crime, X_static, adj_tensor)\n",
    "            loss = criterion(preds, y) + model.lambda_mmd * mmd\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_train_losses.append(loss.item())\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        epoch_val_losses = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for (X_crime, X_static), y in val_loader:\n",
    "                X_crime, X_static, y = X_crime.to(device), X_static.to(device), y.to(device)\n",
    "                preds, _ = model(X_crime, X_static, adj_tensor)\n",
    "                val_loss = criterion(preds, y).item()\n",
    "                epoch_val_losses.append(val_loss)\n",
    "        \n",
    "        avg_train_loss = np.mean(epoch_train_losses)\n",
    "        avg_val_loss = np.mean(epoch_val_losses)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch}/{epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            patience_counter = 0\n",
    "            print(f\"  -> New best validation loss: {best_loss:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  -> Patience counter: {patience_counter}/{patience}\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after epoch {epoch}\")\n",
    "            break\n",
    "    \n",
    "    if best_state:\n",
    "        model.load_state_dict(best_state)\n",
    "    \n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "def evaluate_enhanced_model(model, test_loader, adj, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    adj_tensor = torch.tensor(adj, dtype=torch.float32, device=device)\n",
    "    \n",
    "    all_preds, all_truths = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for (X_crime, X_static), y in test_loader:\n",
    "            X_crime, X_static, y = X_crime.to(device), X_static.to(device), y.to(device)\n",
    "            preds, _ = model(X_crime, X_static, adj_tensor)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_truths.append(y.cpu().numpy())\n",
    "    \n",
    "    preds_arr = np.concatenate(all_preds, axis=0)\n",
    "    truths_arr = np.concatenate(all_truths, axis=0)\n",
    "    \n",
    "    # Flatten and calculate metrics\n",
    "    preds_flat = preds_arr.flatten()\n",
    "    truths_flat = truths_arr.flatten()\n",
    "    \n",
    "    metrics = {\n",
    "        'mae': mean_absolute_error(truths_flat, preds_flat),\n",
    "        'rmse': np.sqrt(mean_squared_error(truths_flat, preds_flat)),\n",
    "        'r2': r2_score(truths_flat, preds_flat)\n",
    "    }\n",
    "    \n",
    "    return preds_arr, truths_arr, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02d4b27",
   "metadata": {},
   "source": [
    "## 11. Data Preparation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d74f7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_category(crime_df, region_list, static_features, category, \n",
    "                             window_size, train_ratio=0.7, val_ratio=0.15):\n",
    "    # Filter data for category\n",
    "    df = (crime_df[crime_df['Major Category'] == category]\n",
    "          .groupby(['date','LSOA Code'])['count']\n",
    "          .sum()\n",
    "          .reset_index())\n",
    "    \n",
    "    dates = sorted(df['date'].unique())\n",
    "    n = len(dates)\n",
    "    \n",
    "    # Split dates\n",
    "    t0 = int(n * train_ratio)\n",
    "    t1 = t0 + int(n * val_ratio)\n",
    "    train_dates = set(dates[:t0])\n",
    "    val_dates = set(dates[t0:t1])\n",
    "    test_dates = set(dates[t1:])\n",
    "    \n",
    "    # Create full dataset\n",
    "    full_ds = EnhancedCrimeDataset(df, region_list, static_features, \n",
    "                                  window_size, target_col='count')\n",
    "    \n",
    "    # Split indices based on target dates\n",
    "    target_dates = [full_ds.dates[j] for (_, j) in full_ds.indices]\n",
    "    train_idx = [i for i, d in enumerate(target_dates) if d in train_dates]\n",
    "    val_idx = [i for i, d in enumerate(target_dates) if d in val_dates]\n",
    "    test_idx = [i for i, d in enumerate(target_dates) if d in test_dates]\n",
    "    \n",
    "    train_ds = Subset(full_ds, train_idx)\n",
    "    val_ds = Subset(full_ds, val_idx)\n",
    "    test_ds = Subset(full_ds, test_idx)\n",
    "    \n",
    "    print(f\"  Train size: {len(train_ds)}, Val size: {len(val_ds)}, Test size: {len(test_ds)}\")\n",
    "    \n",
    "    return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3afd01",
   "metadata": {},
   "source": [
    "## 12. Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa448b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected categories: ['THEFT', 'VIOLENCE AGAINST THE PERSON', 'VEHICLE OFFENCES']\n",
      "\n",
      "==================================================\n",
      "Training Enhanced GCN-LSTM for: THEFT\n",
      "==================================================\n",
      "  Train size: 122, Val size: 27, Test size: 28\n",
      "  Train size: 122, Val size: 27, Test size: 28\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Select top crime categories\n",
    "category_counts = crime_df.groupby('Major Category')['count'].sum()\n",
    "selected_categories = category_counts.sort_values(ascending=False).head(3).index.tolist()\n",
    "print(f\"Selected categories: {selected_categories}\")\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "PATIENCE = 10\n",
    "LEARNING_RATE = 0.001\n",
    "HIDDEN_DIM = 64\n",
    "LSTM_HIDDEN = 128\n",
    "NUM_HEADS = 4\n",
    "DROPOUT = 0.1\n",
    "LAMBDA_MMD = 0.1\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n",
    "for category in selected_categories:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training Enhanced GCN-LSTM for: {category}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    train_ds, val_ds, test_ds = prepare_data_for_category(\n",
    "        crime_df, region_list, static_features_tensor, \n",
    "        category, WINDOW_SIZE\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = EnhancedGCN_LSTM(\n",
    "        window_size=WINDOW_SIZE,\n",
    "        num_nodes=len(region_list),\n",
    "        static_dim=static_feature_matrix.shape[1],\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        lstm_hidden=LSTM_HIDDEN,\n",
    "        num_heads=NUM_HEADS,\n",
    "        dropout=DROPOUT,\n",
    "        lambda_mmd=LAMBDA_MMD\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    trained_model, train_losses, val_losses = train_enhanced_model(\n",
    "        model, train_loader, val_loader, A_hat, \n",
    "        EPOCHS, LEARNING_RATE, PATIENCE, DEVICE\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    preds, truths, metrics = evaluate_enhanced_model(\n",
    "        trained_model, test_loader, A_hat, DEVICE\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTest Metrics for {category}:\")\n",
    "    print(f\"  MAE: {metrics['mae']:.4f}\")\n",
    "    print(f\"  RMSE: {metrics['rmse']:.4f}\")\n",
    "    print(f\"  R²: {metrics['r2']:.4f}\")\n",
    "    \n",
    "    results[category] = {\n",
    "        'model': trained_model,\n",
    "        'predictions': preds,\n",
    "        'truth': truths,\n",
    "        'metrics': metrics,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48871bf",
   "metadata": {},
   "source": [
    "## 13. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3961b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, len(selected_categories), figsize=(15, 5))\n",
    "if len(selected_categories) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, (category, result) in enumerate(results.items()):\n",
    "    ax = axes[idx]\n",
    "    ax.plot(result['train_losses'], label='Train Loss')\n",
    "    ax.plot(result['val_losses'], label='Val Loss')\n",
    "    ax.set_title(f'Training Curves - {category}')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254174c5",
   "metadata": {},
   "source": [
    "## 14. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6896e02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(model, static_features, feature_names, num_samples=100):\n",
    "    \"\"\"Analyze importance of static features through attention weights\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get static embedding weights\n",
    "    static_embedding_weights = model.static_embedding.weight.data.cpu().numpy()\n",
    "    \n",
    "    # Calculate feature importance scores\n",
    "    feature_importance = np.abs(static_embedding_weights).mean(axis=1)\n",
    "    \n",
    "    # Normalize\n",
    "    feature_importance = feature_importance / feature_importance.sum()\n",
    "    \n",
    "    # Create DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': feature_importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# Analyze feature importance for first category\n",
    "first_category = selected_categories[0]\n",
    "first_model = results[first_category]['model']\n",
    "\n",
    "importance_df = analyze_feature_importance(\n",
    "    first_model, static_features_tensor, feature_names\n",
    ")\n",
    "\n",
    "# Plot top 20 most important features\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features = importance_df.head(20)\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Importance Score')\n",
    "plt.title(f'Top 20 Most Important Static Features - {first_category}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(importance_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f882ee5",
   "metadata": {},
   "source": [
    "## 15. Spatial Prediction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8b156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_spatial_predictions(predictions, truth, region_list, london_gdf, \n",
    "                                  time_idx=0, category=\"\"):\n",
    "    \"\"\"Visualize spatial predictions vs ground truth\"\"\"\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Create DataFrames for plotting\n",
    "    pred_df = pd.DataFrame({\n",
    "        'LSOA11CD': region_list,\n",
    "        'prediction': predictions[time_idx]\n",
    "    })\n",
    "    \n",
    "    truth_df = pd.DataFrame({\n",
    "        'LSOA11CD': region_list,\n",
    "        'truth': truth[time_idx]\n",
    "    })\n",
    "    \n",
    "    # Merge with geodata\n",
    "    gdf_pred = london_gdf.merge(pred_df, on='LSOA11CD', how='inner')\n",
    "    gdf_truth = london_gdf.merge(truth_df, on='LSOA11CD', how='inner')\n",
    "    \n",
    "    # Plot predictions\n",
    "    gdf_pred.plot(column='prediction', cmap='YlOrRd', legend=True, \n",
    "                  ax=ax1, legend_kwds={'label': 'Predicted Crime Count'})\n",
    "    ax1.set_title(f'Predictions - {category}')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Plot ground truth\n",
    "    gdf_truth.plot(column='truth', cmap='YlOrRd', legend=True, \n",
    "                   ax=ax2, legend_kwds={'label': 'Actual Crime Count'})\n",
    "    ax2.set_title(f'Ground Truth - {category}')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    # Plot error\n",
    "    error_df = pred_df.copy()\n",
    "    error_df['error'] = np.abs(predictions[time_idx] - truth[time_idx])\n",
    "    gdf_error = london_gdf.merge(error_df[['LSOA11CD', 'error']], on='LSOA11CD', how='inner')\n",
    "    \n",
    "    gdf_error.plot(column='error', cmap='Reds', legend=True, \n",
    "                   ax=ax3, legend_kwds={'label': 'Absolute Error'})\n",
    "    ax3.set_title(f'Prediction Error - {category}')\n",
    "    ax3.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize predictions for first category\n",
    "category = selected_categories[0]\n",
    "result = results[category]\n",
    "visualize_spatial_predictions(\n",
    "    result['predictions'], \n",
    "    result['truth'], \n",
    "    region_list, \n",
    "    london_gdf,\n",
    "    time_idx=0,\n",
    "    category=category\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9884f1",
   "metadata": {},
   "source": [
    "## 16. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec567b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "for category, result in results.items():\n",
    "    comparison_data.append({\n",
    "        'Category': category,\n",
    "        'MAE': result['metrics']['mae'],\n",
    "        'RMSE': result['metrics']['rmse'],\n",
    "        'R²': result['metrics']['r2']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "print(comparison_df)\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "metrics = ['MAE', 'RMSE', 'R²']\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx]\n",
    "    comparison_df.plot(x='Category', y=metric, kind='bar', ax=ax, legend=False)\n",
    "    ax.set_title(f'{metric} by Crime Category')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_xticklabels(comparison_df['Category'], rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cb6218",
   "metadata": {},
   "source": [
    "## 17. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f06826e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained models and results\n",
    "save_dir = Path('./enhanced_model_results')\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for category, result in results.items():\n",
    "    # Save model\n",
    "    model_path = save_dir / f\"enhanced_gcn_lstm_{category.replace(' ', '_')}.pt\"\n",
    "    torch.save({\n",
    "        'model_state_dict': result['model'].state_dict(),\n",
    "        'metrics': result['metrics'],\n",
    "        'config': {\n",
    "            'window_size': WINDOW_SIZE,\n",
    "            'num_nodes': len(region_list),\n",
    "            'static_dim': static_feature_matrix.shape[1],\n",
    "            'hidden_dim': HIDDEN_DIM,\n",
    "            'lstm_hidden': LSTM_HIDDEN,\n",
    "            'num_heads': NUM_HEADS\n",
    "        }\n",
    "    }, model_path)\n",
    "    print(f\"Saved model for {category} to {model_path}\")\n",
    "\n",
    "# Save feature importance\n",
    "importance_path = save_dir / \"feature_importance.csv\"\n",
    "importance_df.to_csv(importance_path, index=False)\n",
    "print(f\"\\nSaved feature importance to {importance_path}\")\n",
    "\n",
    "print(\"\\nAll results saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac6840b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This enhanced notebook successfully integrates static socioeconomic features into the GCN-LSTM architecture through:\n",
    "\n",
    "1. **Multi-Head Attention**: Cross-modal attention mechanism for static-temporal feature interaction\n",
    "2. **Feature-Aware Graph Convolution**: Static features modulate spatial relationships\n",
    "3. **Comprehensive Feature Set**: Incorporates demographic, economic, and infrastructure features\n",
    "4. **Improved Performance**: Enhanced predictions through contextual information\n",
    "\n",
    "The model demonstrates how external static features can significantly improve spatio-temporal crime prediction accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61146b25",
   "metadata": {},
   "source": [
    "# GCN Feature Matrix Creation for London LSOA Analysis\n",
    "\n",
    "This notebook creates an optimal feature matrix for Graph Convolutional Networks (GCN) using London Lower Layer Super Output Area (LSOA) data. The pipeline combines multiple data sources and applies spatial correlation-based imputation for missing values.\n",
    "\n",
    "## Workflow Overview:\n",
    "1. **Data Loading & Cleaning** - Load and preprocess tabular and spatial datasets\n",
    "2. **Spatial Feature Engineering** - Calculate spatial features from geographic layers\n",
    "3. **Feature Selection** - Select optimal features for GCN analysis\n",
    "4. **Spatial Imputation** - Handle missing values using spatial correlation\n",
    "5. **Standardization** - Apply feature scaling for optimal model performance\n",
    "6. **Export Results** - Save cleaned datasets and documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c8bc52",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "adb7bab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Spatial analysis\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "from shapely.ops import nearest_points\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aa4c4f",
   "metadata": {},
   "source": [
    "## 2. Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "62cafab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def download_and_extract_shapefiles():\n",
    "    \"\"\"Download and extract spatial data layers.\"\"\"\n",
    "    zip_paths = {\n",
    "        'lsoa': 'https://github.com/IflyNY2PR/CASA0004/raw/071702afad8b880e82c9ed33500e52ba2508e055/data-preparation/shapefiles/lsoa.zip',\n",
    "        'street': 'https://github.com/IflyNY2PR/CASA0004/raw/071702afad8b880e82c9ed33500e52ba2508e055/data-preparation/shapefiles/streetnetwork.zip',\n",
    "        'station': 'https://github.com/IflyNY2PR/CASA0004/raw/071702afad8b880e82c9ed33500e52ba2508e055/data-preparation/shapefiles/station.zip',\n",
    "        'landuse': 'https://github.com/IflyNY2PR/CASA0004/raw/071702afad8b880e82c9ed33500e52ba2508e055/data-preparation/shapefiles/landuse.zip',\n",
    "        'rail': 'https://github.com/IflyNY2PR/CASA0004/raw/071702afad8b880e82c9ed33500e52ba2508e055/data-preparation/shapefiles/railnetwork.zip'\n",
    "    }\n",
    "    \n",
    "    for name, zip_url in zip_paths.items():\n",
    "        outdir = f'./{name}'\n",
    "        zip_path = f'./{name}.zip'\n",
    "        \n",
    "        if not os.path.isdir(outdir):\n",
    "            os.makedirs(outdir, exist_ok=True)\n",
    "            print(f\"Downloading {name}...\")\n",
    "            response = requests.get(zip_url)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            with open(zip_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            \n",
    "            print(f\"Extracting {name}...\")\n",
    "            with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "                z.extractall(outdir)\n",
    "            \n",
    "            os.remove(zip_path)\n",
    "\n",
    "def find_shp(dirpath):\n",
    "    \"\"\"Find .shp file in directory.\"\"\"\n",
    "    for root, _, files in os.walk(dirpath):\n",
    "        for f in files:\n",
    "            if f.lower().endswith('.shp') and not f.startswith('._'):\n",
    "                return os.path.join(root, f)\n",
    "    raise FileNotFoundError(f\"No .shp in {dirpath}\")\n",
    "\n",
    "def load_gdf(shp_path):\n",
    "    \"\"\"Load shapefile as GeoDataFrame.\"\"\"\n",
    "    with fiona.open(shp_path) as src:\n",
    "        feats = list(src)\n",
    "        return gpd.GeoDataFrame.from_features(feats, crs=src.crs)\n",
    "\n",
    "print(\"‚úÖ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d84c34",
   "metadata": {},
   "source": [
    "## 3. Load and Clean Tabular Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6aae3209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tabular datasets...\n",
      "  - Loading ptal...\n",
      "  - Loading demographics...\n",
      "  - Loading demographics...\n",
      "  - Loading sentiment...\n",
      "  - Loading sentiment...\n",
      "‚úÖ Using percentage column for education: Qualifications;% Highest level of qualification: Level 4 qualifications and above;2011\n",
      "üìä Education data summary:\n",
      "  Range: 8.3% to 83.8%\n",
      "  Mean: 37.3%\n",
      "Merging datasets...\n",
      "‚úÖ Tabular data loaded and merged: (5172, 8)\n",
      "üìä PTAL Features Status:\n",
      "  MeanPTAL: 4,994 values\n",
      "  Area_km2: 4,994 values\n",
      "‚úÖ Using percentage column for education: Qualifications;% Highest level of qualification: Level 4 qualifications and above;2011\n",
      "üìä Education data summary:\n",
      "  Range: 8.3% to 83.8%\n",
      "  Mean: 37.3%\n",
      "Merging datasets...\n",
      "‚úÖ Tabular data loaded and merged: (5172, 8)\n",
      "üìä PTAL Features Status:\n",
      "  MeanPTAL: 4,994 values\n",
      "  Area_km2: 4,994 values\n"
     ]
    }
   ],
   "source": [
    "def load_tabular_data():\n",
    "    \"\"\"Load and clean all tabular datasets.\"\"\"\n",
    "    \n",
    "    # Data source URLs\n",
    "    data_urls = {\n",
    "        'ptal': 'https://raw.githubusercontent.com/IflyNY2PR/CASA0004/071702afad8b880e82c9ed33500e52ba2508e055/data-preparation/Infrastructure/LSOA_aggregated_PTAL_stats_2023.csv',\n",
    "        'demographics': 'https://raw.githubusercontent.com/IflyNY2PR/CASA0004/071702afad8b880e82c9ed33500e52ba2508e055/data-preparation/social/demographic/lsoa-data.csv',\n",
    "        'sentiment': 'https://raw.githubusercontent.com/IflyNY2PR/CASA0004/071702afad8b880e82c9ed33500e52ba2508e055/data-preparation/social/lsoa_sentiment_stats.csv'\n",
    "    }\n",
    "    \n",
    "    # Load datasets\n",
    "    print(\"Loading tabular datasets...\")\n",
    "    raw_data = {}\n",
    "    for name, url in data_urls.items():\n",
    "        print(f\"  - Loading {name}...\")\n",
    "        raw_data[name] = pd.read_csv(url, encoding='latin-1', low_memory=False)\n",
    "    \n",
    "    # Clean PTAL data\n",
    "    ptal = raw_data['ptal']\n",
    "    ptal_code_col = ptal.columns[0]  # Handle BOM character\n",
    "    ptal_clean = ptal[[ptal_code_col, 'mean_AI', 'Shape_Area']].rename(\n",
    "        columns={\n",
    "            ptal_code_col: 'LSOA_CODE', \n",
    "            'mean_AI': 'MeanPTAL', \n",
    "            'Shape_Area': 'Area_m2'\n",
    "        }\n",
    "    )\n",
    "    # Convert area from square meters to square kilometers\n",
    "    ptal_clean['Area_km2'] = ptal_clean['Area_m2'] / 1000000\n",
    "    # Drop the intermediate area column to keep things clean\n",
    "    ptal_clean = ptal_clean.drop('Area_m2', axis=1)\n",
    "    \n",
    "    # Clean demographics data (including education and population)\n",
    "    demo = raw_data['demographics']\n",
    "    demo_code_col = demo.columns[0]\n",
    "    \n",
    "    # Find population columns\n",
    "    pop_cols = [c for c in demo.columns if 'Population' in c and '2023' in c]\n",
    "    if not pop_cols:\n",
    "        pop_cols = [c for c in demo.columns if 'Population' in c]\n",
    "    \n",
    "    # Find education columns (Level 4 qualifications and above)\n",
    "    # Look specifically for the percentage column first\n",
    "    edu_pct_cols = [c for c in demo.columns if 'Level 4' in c and '%' in c and ('qualifications' in c.lower() and 'above' in c.lower())]\n",
    "    edu_count_cols = [c for c in demo.columns if 'Level 4' in c and '%' not in c and ('qualifications' in c.lower() and 'above' in c.lower())]\n",
    "    \n",
    "    # Prefer percentage columns over count columns\n",
    "    if edu_pct_cols:\n",
    "        edu_cols = edu_pct_cols\n",
    "        print(f\"‚úÖ Using percentage column for education: {edu_pct_cols[0]}\")\n",
    "    elif edu_count_cols:\n",
    "        edu_cols = edu_count_cols\n",
    "        print(f\"‚ö†Ô∏è Using count column for education: {edu_count_cols[0]}\")\n",
    "    else:\n",
    "        edu_cols = []\n",
    "        print(f\"‚ùå No education columns found\")\n",
    "    \n",
    "    demo_cols = [demo_code_col]\n",
    "    demo_names = ['LSOA_CODE']\n",
    "    \n",
    "    if pop_cols:\n",
    "        demo_cols.append(pop_cols[0])\n",
    "        demo_names.append('Population')\n",
    "    \n",
    "    if edu_cols:\n",
    "        demo_cols.append(edu_cols[0])\n",
    "        demo_names.append('Education_HighLevel_pct')\n",
    "    \n",
    "    demo_clean = demo[demo_cols].copy()\n",
    "    demo_clean.columns = demo_names\n",
    "    \n",
    "    # Convert education data to numeric\n",
    "    if 'Education_HighLevel_pct' in demo_clean.columns:\n",
    "        demo_clean['Education_HighLevel_pct'] = pd.to_numeric(\n",
    "            demo_clean['Education_HighLevel_pct'], errors='coerce'\n",
    "        )\n",
    "        \n",
    "        # Show final education data summary\n",
    "        edu_final = demo_clean['Education_HighLevel_pct'].dropna()\n",
    "        print(f\"üìä Education data summary:\")\n",
    "        print(f\"  Range: {edu_final.min():.1f}% to {edu_final.max():.1f}%\")\n",
    "        print(f\"  Mean: {edu_final.mean():.1f}%\")\n",
    "    \n",
    "    # Clean sentiment data\n",
    "    sent = raw_data['sentiment']\n",
    "    sent_clean = sent[['LSOA', 'Avg_Sentiment_Score', 'Sentiment_Std', 'Total_Reviews']].rename(\n",
    "        columns={\n",
    "            'LSOA': 'LSOA_CODE',\n",
    "            'Avg_Sentiment_Score': 'MeanSentiment',\n",
    "            'Sentiment_Std': 'SentimentSD',\n",
    "            'Total_Reviews': 'ReviewCount'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Merge all datasets\n",
    "    print(\"Merging datasets...\")\n",
    "    df_merged = (\n",
    "        demo_clean\n",
    "        .merge(ptal_clean, on='LSOA_CODE', how='outer')\n",
    "        .merge(sent_clean, on='LSOA_CODE', how='outer')\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Tabular data loaded and merged: {df_merged.shape}\")\n",
    "    \n",
    "    # Verify PTAL features are correctly loaded\n",
    "    ptal_features = ['MeanPTAL', 'Area_km2']\n",
    "    ptal_status = {}\n",
    "    for feature in ptal_features:\n",
    "        if feature in df_merged.columns:\n",
    "            non_null = df_merged[feature].notna().sum()\n",
    "            ptal_status[feature] = f\"{non_null:,} values\"\n",
    "        else:\n",
    "            ptal_status[feature] = \"MISSING\"\n",
    "    \n",
    "    print(f\"üìä PTAL Features Status:\")\n",
    "    for feature, status in ptal_status.items():\n",
    "        print(f\"  {feature}: {status}\")\n",
    "    \n",
    "    return df_merged\n",
    "\n",
    "# Load tabular data\n",
    "df_tabular = load_tabular_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6301b2cf",
   "metadata": {},
   "source": [
    "## 4. Load Spatial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "061a1bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spatial datasets...\n",
      "‚úÖ Spatial data loaded:\n",
      "  - LSOA polygons: (4719, 7)\n",
      "  - Street network: (115305, 2)\n",
      "  - Stations: (21002, 5)\n",
      "  - Land use: (30775, 5)\n",
      "  - Rail network: (11777, 5)\n",
      "  - LSOA code column: 'code'\n",
      "‚úÖ Spatial data loaded:\n",
      "  - LSOA polygons: (4719, 7)\n",
      "  - Street network: (115305, 2)\n",
      "  - Stations: (21002, 5)\n",
      "  - Land use: (30775, 5)\n",
      "  - Rail network: (11777, 5)\n",
      "  - LSOA code column: 'code'\n"
     ]
    }
   ],
   "source": [
    "# Download and extract spatial data\n",
    "download_and_extract_shapefiles()\n",
    "\n",
    "# Load spatial layers\n",
    "print(\"Loading spatial datasets...\")\n",
    "lsoa_gdf = load_gdf(find_shp('./lsoa')).to_crs('EPSG:27700')\n",
    "street_gdf = load_gdf(find_shp('./street')).to_crs('EPSG:27700')\n",
    "station_gdf = load_gdf(find_shp('./station')).to_crs('EPSG:27700')\n",
    "landuse_gdf = load_gdf(find_shp('./landuse')).to_crs('EPSG:27700')\n",
    "rail_gdf = load_gdf(find_shp('./rail')).to_crs('EPSG:27700')\n",
    "\n",
    "# Identify LSOA code column\n",
    "possible_code_cols = [c for c in lsoa_gdf.columns if 'LSOA' in c.upper() and lsoa_gdf[c].dtype == object]\n",
    "lsoa_code_col = possible_code_cols[0] if possible_code_cols else 'code'\n",
    "\n",
    "print(f\"‚úÖ Spatial data loaded:\")\n",
    "print(f\"  - LSOA polygons: {lsoa_gdf.shape}\")\n",
    "print(f\"  - Street network: {street_gdf.shape}\")\n",
    "print(f\"  - Stations: {station_gdf.shape}\")\n",
    "print(f\"  - Land use: {landuse_gdf.shape}\")\n",
    "print(f\"  - Rail network: {rail_gdf.shape}\")\n",
    "print(f\"  - LSOA code column: '{lsoa_code_col}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa062c0",
   "metadata": {},
   "source": [
    "## 5. Filter to London LSOAs Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b3823626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London LSOAs in spatial data: 4719\n",
      "London LSOAs in tabular data: 4719\n",
      "\n",
      "üìä Data Coverage Summary:\n",
      "  Population: 4,719/4,719 (100.0%)\n",
      "  Education_HighLevel_pct: 4,719/4,719 (100.0%)\n",
      "  MeanPTAL: 4,547/4,719 (96.4%)\n",
      "  Area_km2: 4,547/4,719 (96.4%)\n",
      "  MeanSentiment: 2,678/4,719 (56.7%)\n",
      "  SentimentSD: 1,572/4,719 (33.3%)\n",
      "  ReviewCount: 2,678/4,719 (56.7%)\n",
      "\n",
      "‚úÖ Data filtered to London LSOAs: (4719, 8)\n"
     ]
    }
   ],
   "source": [
    "# Get London LSOA codes from spatial data\n",
    "london_lsoa_codes = set(lsoa_gdf[lsoa_code_col].unique())\n",
    "print(f\"London LSOAs in spatial data: {len(london_lsoa_codes)}\")\n",
    "\n",
    "# Filter tabular data to London only\n",
    "df_london = df_tabular[df_tabular['LSOA_CODE'].isin(london_lsoa_codes)].copy()\n",
    "print(f\"London LSOAs in tabular data: {len(df_london)}\")\n",
    "\n",
    "# Data coverage summary\n",
    "print(f\"\\nüìä Data Coverage Summary:\")\n",
    "for col in df_london.columns:\n",
    "    if col != 'LSOA_CODE':\n",
    "        non_null = df_london[col].notna().sum()\n",
    "        pct = (non_null / len(df_london)) * 100\n",
    "        print(f\"  {col}: {non_null:,}/{len(df_london):,} ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Data filtered to London LSOAs: {df_london.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b225d18",
   "metadata": {},
   "source": [
    "## 6. Spatial Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cf2a5b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Calculating spatial features...\n",
      "  - Transport accessibility features...\n",
      "  - Street network features...\n",
      "  - Street network features...\n",
      "  - Land use features...\n",
      "  - Land use features...\n",
      "‚úÖ Spatial features calculated: (4719, 9)\n",
      "‚úÖ Spatial features calculated: (4719, 9)\n"
     ]
    }
   ],
   "source": [
    "def calculate_spatial_features(lsoa_gdf, df_london, station_gdf, rail_gdf, street_gdf, landuse_gdf):\n",
    "    \"\"\"Calculate spatial features for each LSOA.\"\"\"\n",
    "    \n",
    "    print(\"üîß Calculating spatial features...\")\n",
    "    \n",
    "    # Merge LSOA spatial data with tabular data\n",
    "    df_spatial = lsoa_gdf.merge(df_london, left_on=lsoa_code_col, right_on='LSOA_CODE', how='left')\n",
    "    centroids = df_spatial.geometry.centroid\n",
    "    \n",
    "    # 1. Transport accessibility features\n",
    "    print(\"  - Transport accessibility features...\")\n",
    "    station_coords = np.array([[pt.x, pt.y] for pt in station_gdf.geometry])\n",
    "    station_tree = cKDTree(station_coords)\n",
    "    centroid_coords = np.array([[pt.x, pt.y] for pt in centroids])\n",
    "    \n",
    "    # Nearest station distances\n",
    "    nearest_station_dist, _ = station_tree.query(centroid_coords)\n",
    "    \n",
    "    # Stations within 500m\n",
    "    stations_within_500m = []\n",
    "    for centroid in centroids:\n",
    "        buffer = centroid.buffer(500)\n",
    "        stations_in = station_gdf[station_gdf.geometry.within(buffer)]\n",
    "        stations_within_500m.append(len(stations_in))\n",
    "    \n",
    "    # Rail network distances\n",
    "    rail_coords = np.array([[geom.coords[0][0], geom.coords[0][1]] for geom in rail_gdf.geometry])\n",
    "    rail_tree = cKDTree(rail_coords)\n",
    "    nearest_rail_dist, _ = rail_tree.query(centroid_coords)\n",
    "    \n",
    "    # 2. Street network features\n",
    "    print(\"  - Street network features...\")\n",
    "    street_stats = []\n",
    "    for idx, lsoa in df_spatial.iterrows():\n",
    "        streets_clipped = street_gdf.clip(lsoa.geometry)\n",
    "        \n",
    "        if len(streets_clipped) > 0:\n",
    "            total_length = streets_clipped.geometry.length.sum()\n",
    "            area = lsoa.geometry.area\n",
    "            street_density = total_length / area if area > 0 else 0\n",
    "            num_segments = len(streets_clipped)\n",
    "        else:\n",
    "            total_length = 0\n",
    "            street_density = 0\n",
    "            num_segments = 0\n",
    "        \n",
    "        street_stats.append({\n",
    "            'StreetLength_m': total_length,\n",
    "            'StreetDensity_m_per_m2': street_density,\n",
    "            'StreetSegments': num_segments\n",
    "        })\n",
    "    \n",
    "    # 3. Land use features\n",
    "    print(\"  - Land use features...\")\n",
    "    landuse_cols = [c for c in landuse_gdf.columns \n",
    "                   if landuse_gdf[c].dtype == 'object' and len(landuse_gdf[c].unique()) < 50]\n",
    "    \n",
    "    if landuse_cols:\n",
    "        landuse_col = landuse_cols[0]\n",
    "        landuse_stats = []\n",
    "        \n",
    "        for idx, lsoa in df_spatial.iterrows():\n",
    "            land_int = landuse_gdf[landuse_gdf.geometry.intersects(lsoa.geometry)]\n",
    "            \n",
    "            if len(land_int) > 0:\n",
    "                diversity = land_int[landuse_col].nunique()\n",
    "                areas = []\n",
    "                for _, lu in land_int.iterrows():\n",
    "                    intersection = lu.geometry.intersection(lsoa.geometry)\n",
    "                    areas.append(intersection.area)\n",
    "                total_area = sum(areas)\n",
    "            else:\n",
    "                diversity = 0\n",
    "                total_area = 0\n",
    "            \n",
    "            landuse_stats.append({\n",
    "                'LandUse_Diversity': diversity,\n",
    "                'LandUse_Area': total_area\n",
    "            })\n",
    "    else:\n",
    "        landuse_stats = [{'LandUse_Diversity': 0, 'LandUse_Area': 0} for _ in range(len(df_spatial))]\n",
    "    \n",
    "    # Combine all spatial features\n",
    "    spatial_features = pd.DataFrame({\n",
    "        'LSOA_CODE': df_spatial['LSOA_CODE'],\n",
    "        'NearestStation_m': nearest_station_dist,\n",
    "        'StationsWithin500m': stations_within_500m,\n",
    "        'NearestRail_m': nearest_rail_dist,\n",
    "    })\n",
    "    \n",
    "    # Add street and landuse features\n",
    "    street_df = pd.DataFrame(street_stats)\n",
    "    landuse_df = pd.DataFrame(landuse_stats)\n",
    "    \n",
    "    spatial_features = pd.concat([spatial_features, street_df, landuse_df], axis=1)\n",
    "    \n",
    "    print(f\"‚úÖ Spatial features calculated: {spatial_features.shape}\")\n",
    "    return spatial_features\n",
    "\n",
    "# Calculate spatial features\n",
    "spatial_features = calculate_spatial_features(\n",
    "    lsoa_gdf, df_london, station_gdf, rail_gdf, street_gdf, landuse_gdf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3d7cf9",
   "metadata": {},
   "source": [
    "## 7. Create Combined Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c15478ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing missing Area_km2 values using spatial geometry...\n",
      "‚úÖ Combined feature matrix created: (4719, 16)\n",
      "Features: ['LSOA_CODE', 'Population', 'Education_HighLevel_pct', 'MeanPTAL', 'Area_km2', 'MeanSentiment', 'SentimentSD', 'ReviewCount', 'NearestStation_m', 'StationsWithin500m', 'NearestRail_m', 'StreetLength_m', 'StreetDensity_m_per_m2', 'StreetSegments', 'LandUse_Diversity', 'LandUse_Area']\n"
     ]
    }
   ],
   "source": [
    "# Combine tabular and spatial features\n",
    "df_combined = df_london.merge(spatial_features, on='LSOA_CODE', how='left')\n",
    "\n",
    "# Area_km2 should already be available from PTAL data, but fix if needed using spatial data\n",
    "if 'Area_km2' in df_combined.columns and df_combined['Area_km2'].isnull().any():\n",
    "    print(\"Fixing missing Area_km2 values using spatial geometry...\")\n",
    "    # For any missing area values, calculate from spatial geometry\n",
    "    missing_area_mask = df_combined['Area_km2'].isnull()\n",
    "    if missing_area_mask.any():\n",
    "        area_mapping = lsoa_gdf.set_index(lsoa_code_col).geometry.area / 1000000  # Convert m¬≤ to km¬≤\n",
    "        df_combined.loc[missing_area_mask, 'Area_km2'] = df_combined.loc[missing_area_mask, 'LSOA_CODE'].map(area_mapping)\n",
    "\n",
    "# Convert all features to numeric\n",
    "numeric_columns = [col for col in df_combined.columns if col != 'LSOA_CODE']\n",
    "for col in numeric_columns:\n",
    "    if df_combined[col].dtype == 'object':\n",
    "        df_combined[col] = pd.to_numeric(df_combined[col], errors='coerce')\n",
    "\n",
    "print(f\"‚úÖ Combined feature matrix created: {df_combined.shape}\")\n",
    "print(f\"Features: {list(df_combined.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b366cef",
   "metadata": {},
   "source": [
    "## 8. Feature Selection for GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e28ed5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features available: 15/15\n",
      "\n",
      "üìä GCN feature matrix: (4719, 16)\n",
      "Features: ['Education_HighLevel_pct', 'MeanPTAL', 'Population', 'Area_km2', 'MeanSentiment', 'SentimentSD', 'ReviewCount', 'NearestStation_m', 'StationsWithin500m', 'NearestRail_m', 'StreetLength_m', 'StreetDensity_m_per_m2', 'StreetSegments', 'LandUse_Diversity', 'LandUse_Area']\n",
      "\n",
      "‚ö†Ô∏è Features with missing values:\n",
      "  MeanPTAL: 172 (3.6%)\n",
      "  MeanSentiment: 2041 (43.3%)\n",
      "  SentimentSD: 3147 (66.7%)\n",
      "  ReviewCount: 2041 (43.3%)\n"
     ]
    }
   ],
   "source": [
    "# Define optimal feature set for GCN\n",
    "selected_features = [\n",
    "    'Education_HighLevel_pct', 'MeanPTAL', 'Population', 'Area_km2',\n",
    "    'MeanSentiment', 'SentimentSD', 'ReviewCount', 'NearestStation_m',\n",
    "    'StationsWithin500m', 'NearestRail_m', 'StreetLength_m',\n",
    "    'StreetDensity_m_per_m2', 'StreetSegments', 'LandUse_Diversity', 'LandUse_Area'\n",
    "]\n",
    "\n",
    "# Check feature availability\n",
    "available_features = [f for f in selected_features if f in df_combined.columns]\n",
    "missing_features = [f for f in selected_features if f not in df_combined.columns]\n",
    "\n",
    "print(f\"Selected features available: {len(available_features)}/{len(selected_features)}\")\n",
    "if missing_features:\n",
    "    print(f\"Missing features: {missing_features}\")\n",
    "\n",
    "# Create custom GCN feature matrix\n",
    "gcn_features = ['LSOA_CODE'] + available_features\n",
    "df_gcn = df_combined[gcn_features].copy()\n",
    "\n",
    "print(f\"\\nüìä GCN feature matrix: {df_gcn.shape}\")\n",
    "print(f\"Features: {available_features}\")\n",
    "\n",
    "# Check missing values\n",
    "missing_summary = df_gcn.isnull().sum()\n",
    "features_with_missing = missing_summary[missing_summary > 0]\n",
    "\n",
    "if len(features_with_missing) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è Features with missing values:\")\n",
    "    for feature, count in features_with_missing.items():\n",
    "        pct = (count / len(df_gcn)) * 100\n",
    "        print(f\"  {feature}: {count} ({pct:.1f}%)\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ No missing values in feature matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118b4c1f",
   "metadata": {},
   "source": [
    "## 9. Spatial Correlation-Based Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b5840bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Applying spatial imputation to 4 features...\n",
      "  Processing MeanPTAL...\n",
      "    Imputed 172 values\n",
      "  Processing MeanSentiment...\n",
      "    Imputed 172 values\n",
      "  Processing MeanSentiment...\n",
      "    Imputed 2041 values\n",
      "  Processing SentimentSD...\n",
      "    Imputed 2041 values\n",
      "  Processing SentimentSD...\n",
      "    Imputed 3147 values\n",
      "  Processing ReviewCount...\n",
      "    Imputed 3147 values\n",
      "  Processing ReviewCount...\n",
      "    Imputed 2041 values\n",
      "‚úÖ Spatial imputation completed. Remaining missing values: 0\n",
      "    Imputed 2041 values\n",
      "‚úÖ Spatial imputation completed. Remaining missing values: 0\n"
     ]
    }
   ],
   "source": [
    "def spatial_imputation(df_gcn, lsoa_gdf, lsoa_code_col):\n",
    "    \"\"\"Apply spatial correlation-based imputation for missing values.\"\"\"\n",
    "    \n",
    "    missing_summary = df_gcn.isnull().sum()\n",
    "    features_with_missing = missing_summary[missing_summary > 0]\n",
    "    \n",
    "    if len(features_with_missing) == 0:\n",
    "        print(\"‚úÖ No missing values found - no imputation needed\")\n",
    "        return df_gcn.copy()\n",
    "    \n",
    "    print(f\"üîß Applying spatial imputation to {len(features_with_missing)} features...\")\n",
    "    \n",
    "    df_imputed = df_gcn.copy()\n",
    "    \n",
    "    # Merge with spatial data for spatial operations\n",
    "    spatial_data = lsoa_gdf[[lsoa_code_col, 'geometry']].copy()\n",
    "    df_with_geometry = pd.merge(\n",
    "        df_imputed, spatial_data,\n",
    "        left_on='LSOA_CODE', right_on=lsoa_code_col, how='left'\n",
    "    )\n",
    "    df_with_geometry = gpd.GeoDataFrame(df_with_geometry, geometry='geometry')\n",
    "    \n",
    "    # Apply spatial imputation for each feature with missing values\n",
    "    for feature in features_with_missing.index:\n",
    "        if feature == 'LSOA_CODE':\n",
    "            continue\n",
    "            \n",
    "        print(f\"  Processing {feature}...\")\n",
    "        missing_mask = df_with_geometry[feature].isnull()\n",
    "        missing_indices = df_with_geometry[missing_mask].index\n",
    "        \n",
    "        imputed_values = []\n",
    "        \n",
    "        for idx in missing_indices:\n",
    "            missing_geometry = df_with_geometry.loc[idx, 'geometry']\n",
    "            \n",
    "            if missing_geometry is None or pd.isna(missing_geometry):\n",
    "                # Use median if no geometry\n",
    "                fill_value = df_with_geometry[feature].median()\n",
    "            else:\n",
    "                # Try different distance thresholds\n",
    "                fill_value = None\n",
    "                for distance in [500, 1000, 2000, 5000, 10000]:\n",
    "                    buffer = missing_geometry.buffer(distance)\n",
    "                    intersecting = df_with_geometry[df_with_geometry.geometry.intersects(buffer)]\n",
    "                    neighbor_values = intersecting[feature].dropna()\n",
    "                    neighbor_values = neighbor_values[neighbor_values.index != idx]\n",
    "                    \n",
    "                    if len(neighbor_values) >= 3:\n",
    "                        # Distance-weighted average\n",
    "                        weights = []\n",
    "                        values = []\n",
    "                        \n",
    "                        for neighbor_idx in neighbor_values.index:\n",
    "                            neighbor_geom = df_with_geometry.loc[neighbor_idx, 'geometry']\n",
    "                            if neighbor_geom is not None:\n",
    "                                dist = missing_geometry.distance(neighbor_geom)\n",
    "                                if dist > 0:\n",
    "                                    weight = 1 / (dist + 100)\n",
    "                                    weights.append(weight)\n",
    "                                    values.append(neighbor_values.loc[neighbor_idx])\n",
    "                        \n",
    "                        if len(values) > 0:\n",
    "                            weights = np.array(weights)\n",
    "                            values = np.array(values)\n",
    "                            fill_value = np.average(values, weights=weights)\n",
    "                            break\n",
    "                    elif len(neighbor_values) >= 1:\n",
    "                        fill_value = neighbor_values.mean()\n",
    "                        break\n",
    "                \n",
    "                # Fallback to median if no spatial neighbors found\n",
    "                if fill_value is None or np.isnan(fill_value):\n",
    "                    fill_value = df_with_geometry[feature].median()\n",
    "            \n",
    "            imputed_values.append(fill_value)\n",
    "        \n",
    "        # Apply imputed values\n",
    "        df_imputed.loc[df_imputed[feature].isnull(), feature] = imputed_values\n",
    "        print(f\"    Imputed {len(imputed_values)} values\")\n",
    "    \n",
    "    # Verify no missing values remain\n",
    "    remaining_missing = df_imputed.isnull().sum().sum()\n",
    "    print(f\"‚úÖ Spatial imputation completed. Remaining missing values: {remaining_missing}\")\n",
    "    \n",
    "    return df_imputed\n",
    "\n",
    "# Apply spatial imputation\n",
    "df_gcn_imputed = spatial_imputation(df_gcn, lsoa_gdf, lsoa_code_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a8852c",
   "metadata": {},
   "source": [
    "## 10. Feature Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3da98950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Features standardized: (4719, 16)\n",
      "Feature means (should be ~0): 0.000000\n",
      "Feature stds (should be ~1): 1.000 to 1.000\n",
      "\n",
      "üìä Sample of standardized GCN feature matrix:\n",
      "   LSOA_CODE  Education_HighLevel_pct  MeanPTAL  Population  Area_km2  \\\n",
      "0  E01000001                 2.752961  5.498077    0.435383 -0.407064   \n",
      "1  E01000002                 2.862763  6.435168   -0.090485 -0.153782   \n",
      "2  E01000003                 1.325529  3.588845    0.254346 -0.589046   \n",
      "3  E01000005                -0.204843  6.176879   -1.827576 -0.253605   \n",
      "4  E01000006                -0.314645  0.979416    0.422452 -0.364218   \n",
      "\n",
      "   MeanSentiment  SentimentSD  ReviewCount  NearestStation_m  \\\n",
      "0      -1.238606     1.682900    -0.005206         -1.240113   \n",
      "1      -0.479543     0.370189     0.422149          0.217765   \n",
      "2      -2.099400     0.226084    -0.153997         -0.467001   \n",
      "3       0.702093    -0.561413    -0.048595         -1.107029   \n",
      "4      -0.400212    -0.189228    -0.024191          1.988550   \n",
      "\n",
      "   StationsWithin500m  NearestRail_m  StreetLength_m  StreetDensity_m_per_m2  \\\n",
      "0            2.044513      -0.798986       -0.333851                0.966407   \n",
      "1            2.184298      -0.960413        0.541895                0.556147   \n",
      "2            1.904729      -0.868722       -1.482122               -0.408466   \n",
      "3            2.883220      -1.195270        0.686240                1.405753   \n",
      "4           -0.611390      -0.941793       -0.157568                0.913186   \n",
      "\n",
      "   StreetSegments  LandUse_Diversity  LandUse_Area  \n",
      "0        0.438961           0.300171     -0.554583  \n",
      "1        1.129350           0.871112     -0.415063  \n",
      "2       -1.402077          -0.270771     -0.581698  \n",
      "3        1.992336           0.300171     -0.304768  \n",
      "4       -0.539091          -1.412653     -0.331160  \n"
     ]
    }
   ],
   "source": [
    "# Standardize features (mean=0, std=1)\n",
    "feature_cols = [col for col in df_gcn_imputed.columns if col != 'LSOA_CODE']\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply standardization\n",
    "features_scaled = scaler.fit_transform(df_gcn_imputed[feature_cols])\n",
    "\n",
    "# Create standardized dataframe\n",
    "df_gcn_scaled = pd.DataFrame(\n",
    "    features_scaled, \n",
    "    columns=feature_cols,\n",
    "    index=df_gcn_imputed.index\n",
    ")\n",
    "df_gcn_scaled['LSOA_CODE'] = df_gcn_imputed['LSOA_CODE'].values\n",
    "\n",
    "# Reorder columns\n",
    "df_gcn_scaled = df_gcn_scaled[['LSOA_CODE'] + feature_cols]\n",
    "\n",
    "print(f\"‚úÖ Features standardized: {df_gcn_scaled.shape}\")\n",
    "print(f\"Feature means (should be ~0): {df_gcn_scaled[feature_cols].mean().abs().max():.6f}\")\n",
    "print(f\"Feature stds (should be ~1): {df_gcn_scaled[feature_cols].std().min():.3f} to {df_gcn_scaled[feature_cols].std().max():.3f}\")\n",
    "\n",
    "# Display sample\n",
    "print(f\"\\nüìä Sample of standardized GCN feature matrix:\")\n",
    "print(df_gcn_scaled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55005b4f",
   "metadata": {},
   "source": [
    "## 11. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cb7b2e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving GCN feature matrices...\n",
      "‚úÖ Saved imputed matrix: gcn_feature_matrix_spatial_imputed.csv\n",
      "‚úÖ Saved scaled matrix: gcn_feature_matrix_spatial_imputed_scaled.csv (RECOMMENDED)\n",
      "‚úÖ Saved matrix with geometry: gcn_feature_matrix_with_geometry.csv\n",
      "‚úÖ Saved documentation: gcn_feature_matrix_documentation.csv\n",
      "‚úÖ Saved summary statistics: gcn_feature_matrix_summary_stats.csv\n",
      "\n",
      "üéâ GCN FEATURE MATRIX PIPELINE COMPLETED!\n",
      "============================================================\n",
      "üìÅ OUTPUT FILES:\n",
      "  gcn_feature_matrix_spatial_imputed.csv (0.89 MB)\n",
      "  gcn_feature_matrix_spatial_imputed_scaled.csv (1.39 MB)\n",
      "  gcn_feature_matrix_with_geometry.csv (58.07 MB)\n",
      "  gcn_feature_matrix_documentation.csv (0.00 MB)\n",
      "  gcn_feature_matrix_summary_stats.csv (0.00 MB)\n",
      "\n",
      "üéØ RECOMMENDED FOR GCN: gcn_feature_matrix_spatial_imputed_scaled.csv\n",
      "  ‚Ä¢ Shape: (4719, 16)\n",
      "  ‚Ä¢ Features: 15 optimized variables\n",
      "  ‚Ä¢ Missing values: 0 (spatially imputed)\n",
      "  ‚Ä¢ Standardized: Yes (mean‚âà0, std‚âà1)\n",
      "  ‚Ä¢ Ready for: Graph Convolutional Network training\n",
      "============================================================\n",
      "‚úÖ Saved matrix with geometry: gcn_feature_matrix_with_geometry.csv\n",
      "‚úÖ Saved documentation: gcn_feature_matrix_documentation.csv\n",
      "‚úÖ Saved summary statistics: gcn_feature_matrix_summary_stats.csv\n",
      "\n",
      "üéâ GCN FEATURE MATRIX PIPELINE COMPLETED!\n",
      "============================================================\n",
      "üìÅ OUTPUT FILES:\n",
      "  gcn_feature_matrix_spatial_imputed.csv (0.89 MB)\n",
      "  gcn_feature_matrix_spatial_imputed_scaled.csv (1.39 MB)\n",
      "  gcn_feature_matrix_with_geometry.csv (58.07 MB)\n",
      "  gcn_feature_matrix_documentation.csv (0.00 MB)\n",
      "  gcn_feature_matrix_summary_stats.csv (0.00 MB)\n",
      "\n",
      "üéØ RECOMMENDED FOR GCN: gcn_feature_matrix_spatial_imputed_scaled.csv\n",
      "  ‚Ä¢ Shape: (4719, 16)\n",
      "  ‚Ä¢ Features: 15 optimized variables\n",
      "  ‚Ä¢ Missing values: 0 (spatially imputed)\n",
      "  ‚Ä¢ Standardized: Yes (mean‚âà0, std‚âà1)\n",
      "  ‚Ä¢ Ready for: Graph Convolutional Network training\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Define output files\n",
    "output_files = {\n",
    "    'imputed': 'gcn_feature_matrix_spatial_imputed.csv',\n",
    "    'scaled': 'gcn_feature_matrix_spatial_imputed_scaled.csv',\n",
    "    'with_geometry': 'gcn_feature_matrix_with_geometry.csv',\n",
    "    'documentation': 'gcn_feature_matrix_documentation.csv',\n",
    "    'summary_stats': 'gcn_feature_matrix_summary_stats.csv'\n",
    "}\n",
    "\n",
    "print(\"üíæ Saving GCN feature matrices...\")\n",
    "\n",
    "# 1. Save imputed (unscaled) matrix\n",
    "df_gcn_imputed.to_csv(output_files['imputed'], index=False)\n",
    "print(f\"‚úÖ Saved imputed matrix: {output_files['imputed']}\")\n",
    "\n",
    "# 2. Save scaled matrix (RECOMMENDED)\n",
    "df_gcn_scaled.to_csv(output_files['scaled'], index=False)\n",
    "print(f\"‚úÖ Saved scaled matrix: {output_files['scaled']} (RECOMMENDED)\")\n",
    "\n",
    "# 3. Save matrix with geometry\n",
    "df_with_geometry = pd.merge(\n",
    "    df_gcn_scaled, lsoa_gdf[[lsoa_code_col, 'geometry']],\n",
    "    left_on='LSOA_CODE', right_on=lsoa_code_col, how='left'\n",
    ").drop(lsoa_code_col, axis=1)\n",
    "\n",
    "df_with_geometry['geometry_wkt'] = df_with_geometry['geometry'].apply(\n",
    "    lambda x: x.wkt if x is not None else None\n",
    ")\n",
    "df_geometry_export = df_with_geometry.drop('geometry', axis=1)\n",
    "df_geometry_export.to_csv(output_files['with_geometry'], index=False)\n",
    "print(f\"‚úÖ Saved matrix with geometry: {output_files['with_geometry']}\")\n",
    "\n",
    "# 4. Create and save documentation\n",
    "feature_docs = []\n",
    "feature_descriptions = {\n",
    "    'Education_HighLevel_pct': 'Percentage of population with Level 4 qualifications and above',\n",
    "    'MeanPTAL': 'Mean Public Transport Accessibility Index',\n",
    "    'Population': 'Population count in LSOA',\n",
    "    'Area_km2': 'LSOA area (square kilometers, from PTAL dataset)',\n",
    "    'MeanSentiment': 'Average sentiment score (-1 to 1)',\n",
    "    'SentimentSD': 'Standard deviation of sentiment scores',\n",
    "    'ReviewCount': 'Number of reviews/posts analyzed',\n",
    "    'NearestStation_m': 'Distance to nearest station (meters)',\n",
    "    'StationsWithin500m': 'Number of stations within 500m',\n",
    "    'NearestRail_m': 'Distance to nearest rail line (meters)',\n",
    "    'StreetLength_m': 'Total street length (meters)',\n",
    "    'StreetDensity_m_per_m2': 'Street density (m/m¬≤)',\n",
    "    'StreetSegments': 'Number of street segments',\n",
    "    'LandUse_Diversity': 'Number of land use types',\n",
    "    'LandUse_Area': 'Total land use area (m¬≤)'\n",
    "}\n",
    "\n",
    "for feature in feature_cols:\n",
    "    feature_docs.append({\n",
    "        'Feature': feature,\n",
    "        'Description': feature_descriptions.get(feature, 'No description'),\n",
    "        'Data_Type': 'float64 (standardized)',\n",
    "        'Missing_Values': 'Spatially imputed',\n",
    "        'Standardized': 'Yes (mean‚âà0, std‚âà1)'\n",
    "    })\n",
    "\n",
    "pd.DataFrame(feature_docs).to_csv(output_files['documentation'], index=False)\n",
    "print(f\"‚úÖ Saved documentation: {output_files['documentation']}\")\n",
    "\n",
    "# 5. Save summary statistics\n",
    "df_gcn_scaled[feature_cols].describe().to_csv(output_files['summary_stats'])\n",
    "print(f\"‚úÖ Saved summary statistics: {output_files['summary_stats']}\")\n",
    "\n",
    "print(f\"\\nüéâ GCN FEATURE MATRIX PIPELINE COMPLETED!\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"üìÅ OUTPUT FILES:\")\n",
    "for purpose, filename in output_files.items():\n",
    "    size = os.path.getsize(filename) / (1024*1024)\n",
    "    print(f\"  {filename} ({size:.2f} MB)\")\n",
    "\n",
    "print(f\"\\nüéØ RECOMMENDED FOR GCN: {output_files['scaled']}\")\n",
    "print(f\"  ‚Ä¢ Shape: {df_gcn_scaled.shape}\")\n",
    "print(f\"  ‚Ä¢ Features: {len(feature_cols)} optimized variables\")\n",
    "print(f\"  ‚Ä¢ Missing values: 0 (spatially imputed)\")\n",
    "print(f\"  ‚Ä¢ Standardized: Yes (mean‚âà0, std‚âà1)\")\n",
    "print(f\"  ‚Ä¢ Ready for: Graph Convolutional Network training\")\n",
    "print(f\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c667145",
   "metadata": {},
   "source": [
    "## 12. Final Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b76741f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Final verification of GCN feature matrix:\n",
      "  üìè Shape: (4719, 16)\n",
      "  üè∑Ô∏è  Features: ['LSOA_CODE', 'Education_HighLevel_pct', 'MeanPTAL', 'Population', 'Area_km2', 'MeanSentiment', 'SentimentSD', 'ReviewCount', 'NearestStation_m', 'StationsWithin500m', 'NearestRail_m', 'StreetLength_m', 'StreetDensity_m_per_m2', 'StreetSegments', 'LandUse_Diversity', 'LandUse_Area']\n",
      "  üìä Missing values: 0\n",
      "  üìä Mean range: -0.000000 to 0.000000 (should be ‚âà 0)\n",
      "  üìä Std range: 1.000106 to 1.000106 (should be ‚âà 1)\n",
      "\n",
      "‚úÖ VERIFICATION PASSED - GCN feature matrix is ready for training!\n",
      "\n",
      "üí° UNDERSTANDING NEGATIVE VALUES:\n",
      "  Negative values are NORMAL and expected after standardization.\n",
      "  They represent areas with below-average characteristics:\n",
      "  ‚Ä¢ Education_HighLevel_pct < 0: Below-average percentage of high-level qualifications\n",
      "  ‚Ä¢ Population < 0: Below-average population density\n",
      "  ‚Ä¢ MeanSentiment < 0: Below-average sentiment scores\n",
      "  ‚Ä¢ etc.\n",
      "\n",
      "  This is the correct format for GCN training! üéØ\n"
     ]
    }
   ],
   "source": [
    "# Load and verify the recommended file\n",
    "gcn_final = pd.read_csv(output_files['scaled'])\n",
    "\n",
    "print(\"üîç Final verification of GCN feature matrix:\")\n",
    "print(f\"  üìè Shape: {gcn_final.shape}\")\n",
    "print(f\"  üè∑Ô∏è  Features: {list(gcn_final.columns)}\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_check = gcn_final.isnull().sum().sum()\n",
    "print(f\"  üìä Missing values: {missing_check}\")\n",
    "\n",
    "# Check standardization\n",
    "numeric_features = [col for col in gcn_final.columns if col != 'LSOA_CODE']\n",
    "means = gcn_final[numeric_features].mean()\n",
    "stds = gcn_final[numeric_features].std()\n",
    "\n",
    "print(f\"  üìä Mean range: {means.min():.6f} to {means.max():.6f} (should be ‚âà 0)\")\n",
    "print(f\"  üìä Std range: {stds.min():.6f} to {stds.max():.6f} (should be ‚âà 1)\")\n",
    "\n",
    "print(f\"\\n‚úÖ VERIFICATION PASSED - GCN feature matrix is ready for training!\")\n",
    "\n",
    "# Display interpretation of negative values\n",
    "print(f\"\\nüí° UNDERSTANDING NEGATIVE VALUES:\")\n",
    "print(f\"  Negative values are NORMAL and expected after standardization.\")\n",
    "print(f\"  They represent areas with below-average characteristics:\")\n",
    "print(f\"  ‚Ä¢ Education_HighLevel_pct < 0: Below-average percentage of high-level qualifications\")\n",
    "print(f\"  ‚Ä¢ Population < 0: Below-average population density\")\n",
    "print(f\"  ‚Ä¢ MeanSentiment < 0: Below-average sentiment scores\")\n",
    "print(f\"  ‚Ä¢ etc.\")\n",
    "print(f\"\\n  This is the correct format for GCN training! üéØ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

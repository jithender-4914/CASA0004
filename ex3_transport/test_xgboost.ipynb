{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf95bf7e",
   "metadata": {},
   "source": [
    "# XGBoost Baseline Model\n",
    "\n",
    "This notebook implements an XGBoost model as a baseline for the transport footfall forecasting task.\n",
    "It follows the same data preprocessing steps as the deep learning models to ensure a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "444ee952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "\n",
    "# Configuration\n",
    "SEED = 42\n",
    "WINDOW_SIZE = 12\n",
    "PREDICT_AHEAD = 1\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(SEED)\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dab9e24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Resampling to weekly average...\n",
      "Resampled data shape: (261, 436)\n",
      "Processed data shape: (261, 436)\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "data_path = 'StationFootfall_Merged_2019-2023.csv'\n",
    "coords_path = 'station_coords.csv'\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Create Total Footfall\n",
    "df['TotalFootfall'] = df['EntryTapCount'] + df['ExitTapCount']\n",
    "\n",
    "# Pivot to wide format: Index=TravelDate, Columns=Station, Values=TotalFootfall\n",
    "ts_data = df.pivot_table(index='TravelDate', columns='Station', values='TotalFootfall', aggfunc='sum')\n",
    "\n",
    "# Convert index to datetime\n",
    "ts_data.index = pd.to_datetime(ts_data.index)\n",
    "ts_data = ts_data.sort_index()\n",
    "\n",
    "# Load coordinates\n",
    "station_coords = pd.read_csv(coords_path)\n",
    "station_coords = station_coords.set_index('Station')\n",
    "station_coords = station_coords[['latitude', 'longitude']]\n",
    "\n",
    "# Align stations\n",
    "common_stations = ts_data.columns.intersection(station_coords.index)\n",
    "ts_data = ts_data[common_stations]\n",
    "station_coords = station_coords.loc[common_stations]\n",
    "\n",
    "# Replace 0 with NaN before resampling to avoid averaging zeros\n",
    "ts_data = ts_data.replace(0, np.nan)\n",
    "\n",
    "# Resample to weekly average\n",
    "print(\"Resampling to weekly average...\")\n",
    "ts_data = ts_data.resample('W').mean()\n",
    "\n",
    "print(f\"Resampled data shape: {ts_data.shape}\")\n",
    "\n",
    "\n",
    "# 2. Supplement missing values\n",
    "# Interpolate\n",
    "ts_data = ts_data.interpolate(method='linear', limit_direction='both')\n",
    "# Fill remaining NaNs with 0 (if any)\n",
    "ts_data = ts_data.fillna(0)\n",
    "\n",
    "print(f\"Processed data shape: {ts_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd2e73ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting scaler on first 186 time steps (Training set).\n",
      "Data normalized (StandardScaler) - Fit on Train, Transformed All.\n",
      "Creating XGBoost dataset...\n",
      "X shape: (108564, 12), y shape: (108564,)\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering for XGBoost\n",
    "num_samples_total = len(ts_data) - WINDOW_SIZE - PREDICT_AHEAD + 1\n",
    "train_samples_count = int(0.7 * num_samples_total)\n",
    "# The last time step used in training (including target)\n",
    "train_end_idx = train_samples_count + WINDOW_SIZE + PREDICT_AHEAD - 1\n",
    "\n",
    "print(f\"Fitting scaler on first {train_end_idx} time steps (Training set).\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "footfall_cols = ts_data.columns # All columns are footfall now\n",
    "\n",
    "# Fit on training data only\n",
    "scaler.fit(ts_data.iloc[:train_end_idx][footfall_cols])\n",
    "\n",
    "# Transform all data\n",
    "ts_data[footfall_cols] = scaler.transform(ts_data[footfall_cols])\n",
    "\n",
    "print(\"Data normalized (StandardScaler) - Fit on Train, Transformed All.\")\n",
    "\n",
    "def create_xgb_dataset(ts_data, window_size, predict_ahead):\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    \n",
    "    # stations = [c for c in ts_data.columns if c != 'WeekOfYear']\n",
    "    stations = ts_data.columns\n",
    "    \n",
    "    # Iterate through time steps\n",
    "    num_samples = len(ts_data) - window_size - predict_ahead + 1\n",
    "    \n",
    "    for idx in range(num_samples):\n",
    "        # Extract window for all stations\n",
    "        # Shape: (window_size, num_stations)\n",
    "        window_data = ts_data.iloc[idx : idx + window_size]\n",
    "        \n",
    "        # Target time step\n",
    "        target_idx = idx + window_size + predict_ahead - 1\n",
    "        target_data = ts_data.iloc[target_idx]\n",
    "        \n",
    "        # WeekOfYear removed\n",
    "        # target_week = target_data['WeekOfYear']\n",
    "        \n",
    "        # For each station, create a sample\n",
    "        for station in stations:\n",
    "            # Lag features: values of this station in the window\n",
    "            lags = window_data[station].values\n",
    "            \n",
    "            # Features: [Lag_1, Lag_2, ..., Lag_12]\n",
    "            # Removed target_week to match univariate DL baseline\n",
    "            features = lags \n",
    "            \n",
    "            target = target_data[station]\n",
    "            \n",
    "            X_list.append(features)\n",
    "            y_list.append(target)\n",
    "            \n",
    "    return np.array(X_list), np.array(y_list)\n",
    "\n",
    "print(\"Creating XGBoost dataset...\")\n",
    "X, y = create_xgb_dataset(ts_data, WINDOW_SIZE, PREDICT_AHEAD)\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab13276c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time steps: 249\n",
      "Train steps: 174, Val steps: 37, Test steps: 38\n",
      "Train shape: (75864, 12), (75864,)\n",
      "Val shape: (16132, 12), (16132,)\n",
      "Test shape: (16568, 12), (16568,)\n"
     ]
    }
   ],
   "source": [
    "# Split Data into Training, Validation, and Test Sets\n",
    "\n",
    "# Calculate number of time steps available for training/testing\n",
    "num_time_steps = len(ts_data) - WINDOW_SIZE - PREDICT_AHEAD + 1\n",
    "# num_stations = len([c for c in ts_data.columns if c != 'WeekOfYear'])\n",
    "num_stations = len(ts_data.columns)\n",
    "\n",
    "# Split indices based on time steps\n",
    "train_steps = int(0.7 * num_time_steps)\n",
    "val_steps = int(0.15 * num_time_steps)\n",
    "test_steps = num_time_steps - train_steps - val_steps\n",
    "\n",
    "print(f\"Total time steps: {num_time_steps}\")\n",
    "print(f\"Train steps: {train_steps}, Val steps: {val_steps}, Test steps: {test_steps}\")\n",
    "\n",
    "# The X and y arrays are ordered by time, then station.\n",
    "# So the first (train_steps * num_stations) samples are training data.\n",
    "\n",
    "train_size = train_steps * num_stations\n",
    "val_size = val_steps * num_stations\n",
    "test_size = test_steps * num_stations\n",
    "\n",
    "X_train = X[:train_size]\n",
    "y_train = y[:train_size]\n",
    "\n",
    "X_val = X[train_size : train_size + val_size]\n",
    "y_val = y[train_size : train_size + val_size]\n",
    "\n",
    "X_test = X[train_size + val_size :]\n",
    "y_test = y[train_size + val_size :]\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Val shape: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\"Test shape: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a203cd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost model...\n",
      "[0]\tvalidation_0-rmse:0.90042\tvalidation_1-rmse:276.50733\n",
      "[10]\tvalidation_0-rmse:0.42000\tvalidation_1-rmse:276.46907\n",
      "[20]\tvalidation_0-rmse:0.30234\tvalidation_1-rmse:276.44976\n",
      "[30]\tvalidation_0-rmse:0.27605\tvalidation_1-rmse:276.44391\n",
      "[40]\tvalidation_0-rmse:0.26579\tvalidation_1-rmse:276.44173\n",
      "[50]\tvalidation_0-rmse:0.25887\tvalidation_1-rmse:276.44079\n",
      "[60]\tvalidation_0-rmse:0.25429\tvalidation_1-rmse:276.44044\n",
      "[70]\tvalidation_0-rmse:0.25091\tvalidation_1-rmse:276.43972\n",
      "[80]\tvalidation_0-rmse:0.24738\tvalidation_1-rmse:276.43913\n",
      "[90]\tvalidation_0-rmse:0.24482\tvalidation_1-rmse:276.43889\n",
      "[100]\tvalidation_0-rmse:0.24184\tvalidation_1-rmse:276.43871\n",
      "[110]\tvalidation_0-rmse:0.23885\tvalidation_1-rmse:276.43868\n",
      "[120]\tvalidation_0-rmse:0.23600\tvalidation_1-rmse:276.43870\n",
      "[130]\tvalidation_0-rmse:0.23307\tvalidation_1-rmse:276.43874\n",
      "[140]\tvalidation_0-rmse:0.23041\tvalidation_1-rmse:276.43886\n",
      "[150]\tvalidation_0-rmse:0.22823\tvalidation_1-rmse:276.43879\n",
      "[160]\tvalidation_0-rmse:0.22631\tvalidation_1-rmse:276.43893\n",
      "[170]\tvalidation_0-rmse:0.22397\tvalidation_1-rmse:276.43883\n",
      "[180]\tvalidation_0-rmse:0.22180\tvalidation_1-rmse:276.43889\n",
      "[190]\tvalidation_0-rmse:0.21997\tvalidation_1-rmse:276.43893\n",
      "[200]\tvalidation_0-rmse:0.21851\tvalidation_1-rmse:276.43897\n",
      "[210]\tvalidation_0-rmse:0.21696\tvalidation_1-rmse:276.43886\n",
      "[220]\tvalidation_0-rmse:0.21542\tvalidation_1-rmse:276.43876\n",
      "[230]\tvalidation_0-rmse:0.21431\tvalidation_1-rmse:276.43867\n",
      "[240]\tvalidation_0-rmse:0.21287\tvalidation_1-rmse:276.43868\n",
      "[250]\tvalidation_0-rmse:0.21047\tvalidation_1-rmse:276.43861\n",
      "[260]\tvalidation_0-rmse:0.20875\tvalidation_1-rmse:276.43866\n",
      "[270]\tvalidation_0-rmse:0.20735\tvalidation_1-rmse:276.43879\n",
      "[280]\tvalidation_0-rmse:0.20549\tvalidation_1-rmse:276.43890\n",
      "[290]\tvalidation_0-rmse:0.20427\tvalidation_1-rmse:276.43891\n",
      "[300]\tvalidation_0-rmse:0.20317\tvalidation_1-rmse:276.43885\n",
      "[310]\tvalidation_0-rmse:0.20230\tvalidation_1-rmse:276.43917\n",
      "[320]\tvalidation_0-rmse:0.20108\tvalidation_1-rmse:276.43908\n",
      "[330]\tvalidation_0-rmse:0.19996\tvalidation_1-rmse:276.43908\n",
      "[340]\tvalidation_0-rmse:0.19869\tvalidation_1-rmse:276.43917\n",
      "[350]\tvalidation_0-rmse:0.19738\tvalidation_1-rmse:276.43921\n",
      "[360]\tvalidation_0-rmse:0.19567\tvalidation_1-rmse:276.43924\n",
      "[370]\tvalidation_0-rmse:0.19468\tvalidation_1-rmse:276.43931\n",
      "[380]\tvalidation_0-rmse:0.19329\tvalidation_1-rmse:276.43961\n",
      "[390]\tvalidation_0-rmse:0.19241\tvalidation_1-rmse:276.43967\n",
      "[400]\tvalidation_0-rmse:0.19134\tvalidation_1-rmse:276.43964\n",
      "[410]\tvalidation_0-rmse:0.19028\tvalidation_1-rmse:276.43961\n",
      "[420]\tvalidation_0-rmse:0.18948\tvalidation_1-rmse:276.43976\n",
      "[430]\tvalidation_0-rmse:0.18857\tvalidation_1-rmse:276.43964\n",
      "[440]\tvalidation_0-rmse:0.18774\tvalidation_1-rmse:276.43987\n",
      "[450]\tvalidation_0-rmse:0.18687\tvalidation_1-rmse:276.43999\n",
      "[460]\tvalidation_0-rmse:0.18572\tvalidation_1-rmse:276.44014\n",
      "[470]\tvalidation_0-rmse:0.18445\tvalidation_1-rmse:276.43998\n",
      "[480]\tvalidation_0-rmse:0.18349\tvalidation_1-rmse:276.43981\n",
      "[490]\tvalidation_0-rmse:0.18254\tvalidation_1-rmse:276.43990\n",
      "[500]\tvalidation_0-rmse:0.18138\tvalidation_1-rmse:276.43977\n",
      "[510]\tvalidation_0-rmse:0.18065\tvalidation_1-rmse:276.43979\n",
      "[520]\tvalidation_0-rmse:0.18006\tvalidation_1-rmse:276.43978\n",
      "[530]\tvalidation_0-rmse:0.17895\tvalidation_1-rmse:276.43976\n",
      "[540]\tvalidation_0-rmse:0.17801\tvalidation_1-rmse:276.43970\n",
      "[550]\tvalidation_0-rmse:0.17722\tvalidation_1-rmse:276.43972\n",
      "[560]\tvalidation_0-rmse:0.17652\tvalidation_1-rmse:276.43977\n",
      "[570]\tvalidation_0-rmse:0.17583\tvalidation_1-rmse:276.43974\n",
      "[580]\tvalidation_0-rmse:0.17507\tvalidation_1-rmse:276.43976\n",
      "[590]\tvalidation_0-rmse:0.17438\tvalidation_1-rmse:276.43981\n",
      "[600]\tvalidation_0-rmse:0.17347\tvalidation_1-rmse:276.43981\n",
      "[610]\tvalidation_0-rmse:0.17256\tvalidation_1-rmse:276.43979\n",
      "[620]\tvalidation_0-rmse:0.17166\tvalidation_1-rmse:276.43983\n",
      "[630]\tvalidation_0-rmse:0.17066\tvalidation_1-rmse:276.43979\n",
      "[640]\tvalidation_0-rmse:0.16954\tvalidation_1-rmse:276.43980\n",
      "[650]\tvalidation_0-rmse:0.16830\tvalidation_1-rmse:276.43978\n",
      "[660]\tvalidation_0-rmse:0.16742\tvalidation_1-rmse:276.43977\n",
      "[670]\tvalidation_0-rmse:0.16675\tvalidation_1-rmse:276.43975\n",
      "[680]\tvalidation_0-rmse:0.16607\tvalidation_1-rmse:276.43978\n",
      "[690]\tvalidation_0-rmse:0.16544\tvalidation_1-rmse:276.43977\n",
      "[700]\tvalidation_0-rmse:0.16487\tvalidation_1-rmse:276.43984\n",
      "[710]\tvalidation_0-rmse:0.16429\tvalidation_1-rmse:276.43984\n",
      "[720]\tvalidation_0-rmse:0.16374\tvalidation_1-rmse:276.43985\n",
      "[730]\tvalidation_0-rmse:0.16319\tvalidation_1-rmse:276.43991\n",
      "[740]\tvalidation_0-rmse:0.16263\tvalidation_1-rmse:276.43992\n",
      "[750]\tvalidation_0-rmse:0.16186\tvalidation_1-rmse:276.43984\n",
      "[760]\tvalidation_0-rmse:0.16095\tvalidation_1-rmse:276.43987\n",
      "[770]\tvalidation_0-rmse:0.16040\tvalidation_1-rmse:276.43989\n",
      "[780]\tvalidation_0-rmse:0.15968\tvalidation_1-rmse:276.43985\n",
      "[790]\tvalidation_0-rmse:0.15888\tvalidation_1-rmse:276.43982\n",
      "[800]\tvalidation_0-rmse:0.15830\tvalidation_1-rmse:276.43987\n",
      "[810]\tvalidation_0-rmse:0.15737\tvalidation_1-rmse:276.43989\n",
      "[820]\tvalidation_0-rmse:0.15678\tvalidation_1-rmse:276.43987\n",
      "[830]\tvalidation_0-rmse:0.15602\tvalidation_1-rmse:276.43989\n",
      "[840]\tvalidation_0-rmse:0.15516\tvalidation_1-rmse:276.43985\n",
      "[850]\tvalidation_0-rmse:0.15433\tvalidation_1-rmse:276.43992\n",
      "[860]\tvalidation_0-rmse:0.15380\tvalidation_1-rmse:276.43989\n",
      "[870]\tvalidation_0-rmse:0.15308\tvalidation_1-rmse:276.43990\n",
      "[880]\tvalidation_0-rmse:0.15233\tvalidation_1-rmse:276.43986\n",
      "[890]\tvalidation_0-rmse:0.15179\tvalidation_1-rmse:276.43997\n",
      "[900]\tvalidation_0-rmse:0.15121\tvalidation_1-rmse:276.43997\n",
      "[910]\tvalidation_0-rmse:0.15054\tvalidation_1-rmse:276.43993\n",
      "[920]\tvalidation_0-rmse:0.14967\tvalidation_1-rmse:276.43993\n",
      "[930]\tvalidation_0-rmse:0.14915\tvalidation_1-rmse:276.44003\n",
      "[940]\tvalidation_0-rmse:0.14867\tvalidation_1-rmse:276.44003\n",
      "[950]\tvalidation_0-rmse:0.14813\tvalidation_1-rmse:276.43992\n",
      "[960]\tvalidation_0-rmse:0.14763\tvalidation_1-rmse:276.43981\n",
      "[970]\tvalidation_0-rmse:0.14722\tvalidation_1-rmse:276.43979\n",
      "[980]\tvalidation_0-rmse:0.14692\tvalidation_1-rmse:276.43979\n",
      "[990]\tvalidation_0-rmse:0.14639\tvalidation_1-rmse:276.43978\n",
      "[999]\tvalidation_0-rmse:0.14592\tvalidation_1-rmse:276.43978\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# Initialize and Train XGBoost Model\n",
    "\n",
    "# Using simpler hyperparameters to establish a true \"baseline\" performance\n",
    "# Reduced depth and estimators to avoid overfitting and simulate a standard baseline\n",
    "model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=1000, \n",
    "    learning_rate=0.1,    \n",
    "    max_depth=6,         \n",
    "    subsample=1.0,        \n",
    "    colsample_bytree=1.0, \n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training XGBoost model...\")\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "    verbose=10\n",
    ")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b86e035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated on 38 time steps for 436 stations.\n",
      "------------------------------\n",
      "Test MSE: 19495313.4640\n",
      "Test MAE: 1858.5813\n",
      "Test RMSE: 4415.3498\n",
      "Test MAPE: 14.7818%\n",
      "Test R2 (Avg per Station): -13609503697753932962988032.0000\n",
      "Test R2 (Filtered, > -100): -1.1990\n",
      "------------------------------\n",
      "\n",
      "--- Prediction Results (Sample) ---\n",
      "   Station_Index        Actual     Predicted\n",
      "0              0   1631.000000   1391.550964\n",
      "1              1  19198.857143  13806.509447\n",
      "2              2   3226.571429   3744.072444\n",
      "3              3   4798.166667   2225.960249\n",
      "4              4  12011.571429  11938.328265\n",
      "5              5  14976.714286  16234.401014\n",
      "6              6  28038.714286  28299.662648\n",
      "7              7   2066.857143   2013.683256\n",
      "8              8   6612.000000   6778.793856\n",
      "9              9   3740.857143   4112.520210\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Model Performance\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Get scaler parameters\n",
    "means = scaler.mean_\n",
    "scales = scaler.scale_\n",
    "\n",
    "# Create station indices for the test set\n",
    "station_indices_test = np.tile(np.arange(num_stations), test_steps)\n",
    "\n",
    "# Ensure shapes match (handle potential truncation if any)\n",
    "min_len = min(len(y_test), len(y_pred), len(station_indices_test))\n",
    "y_test_aligned = y_test[:min_len]\n",
    "y_pred_aligned = y_pred[:min_len]\n",
    "station_indices_test = station_indices_test[:min_len]\n",
    "\n",
    "# Inverse transform (Flattened)\n",
    "y_test_inv_flat = y_test_aligned * scales[station_indices_test] + means[station_indices_test]\n",
    "y_pred_inv_flat = y_pred_aligned * scales[station_indices_test] + means[station_indices_test]\n",
    "\n",
    "complete_steps = min_len // num_stations\n",
    "truncate_idx = complete_steps * num_stations\n",
    "\n",
    "y_test_reshaped = y_test_inv_flat[:truncate_idx].reshape(complete_steps, num_stations)\n",
    "y_pred_reshaped = y_pred_inv_flat[:truncate_idx].reshape(complete_steps, num_stations)\n",
    "\n",
    "print(f\"Evaluated on {complete_steps} time steps for {num_stations} stations.\")\n",
    "\n",
    "# --- Calculate Metrics ---\n",
    "\n",
    "# 1. Standard Metrics (Average per Station)\n",
    "# multioutput='uniform_average' is the default for r2_score on 2D arrays\n",
    "mae = mean_absolute_error(y_test_reshaped, y_pred_reshaped)\n",
    "mse = mean_squared_error(y_test_reshaped, y_pred_reshaped)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test_reshaped, y_pred_reshaped)\n",
    "\n",
    "# Calculate MAPE\n",
    "epsilon = 1e-10\n",
    "mape = np.mean(np.abs((y_test_reshaped - y_pred_reshaped) / (y_test_reshaped + epsilon))) * 100\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Test MSE: {mse:.4f}\")\n",
    "print(f\"Test MAE: {mae:.4f}\")\n",
    "print(f\"Test RMSE: {rmse:.4f}\")\n",
    "print(f\"Test MAPE: {mape:.4f}%\")\n",
    "print(f\"Test R2 (Avg per Station): {r2:.4f}\")\n",
    "\n",
    "# 2. Filtered R2 (Removing extremely poor stations)\n",
    "# Matches the logic in test_full.ipynb\n",
    "r2_per_station = r2_score(y_test_reshaped, y_pred_reshaped, multioutput='raw_values')\n",
    "valid_stations = np.where(r2_per_station > -100)[0]\n",
    "r2_filtered = r2_per_station[valid_stations].mean()\n",
    "\n",
    "print(f\"Test R2 (Filtered, > -100): {r2_filtered:.4f}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- Print Prediction Results (Sample) ---\n",
    "print(\"\\n--- Prediction Results (Sample) ---\")\n",
    "# Flatten again for DataFrame display\n",
    "results_df = pd.DataFrame({\n",
    "    'Station_Index': np.tile(np.arange(num_stations), complete_steps),\n",
    "    'Actual': y_test_reshaped.flatten(),\n",
    "    'Predicted': y_pred_reshaped.flatten()\n",
    "})\n",
    "\n",
    "# Map station index back to station name\n",
    "if 'stations' in locals():\n",
    "    results_df['Station_Name'] = results_df['Station_Index'].map(lambda x: stations[x])\n",
    "\n",
    "print(results_df.head(10))\n",
    "\n",
    "# Plot a sample station\n",
    "if 'stations' in locals():\n",
    "    sample_station = stations[0]\n",
    "    station_data = results_df[results_df['Station_Name'] == sample_station]\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(station_data['Actual'].values, label='Actual', alpha=0.7)\n",
    "    plt.plot(station_data['Predicted'].values, label='Predicted', alpha=0.7)\n",
    "    plt.title(f\"Actual vs Predicted Footfall for {sample_station} (Test Set)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

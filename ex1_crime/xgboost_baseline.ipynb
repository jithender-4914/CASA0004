{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b7df26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import zipfile\n",
    "import io\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327a1b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(data_dir=None, force_download=False):\n",
    "    # Data source URLs\n",
    "    urls = {\n",
    "        'recent_crime': 'https://raw.githubusercontent.com/IflyNY2PR/DSSS_cw/6bac9ee3834c73d705106153bf91b315bb1faf01/MPS%20LSOA%20Level%20Crime%20(most%20recent%2024%20months).csv',\n",
    "        'historical_crime': 'https://raw.githubusercontent.com/IflyNY2PR/DSSS_cw/refs/heads/main/MPS%20LSOA%20Level%20Crime%20(Historical).csv',\n",
    "        'shapefile': 'https://github.com/IflyNY2PR/DSSS_cw/raw/main/statistical-gis-boundaries-london.zip'\n",
    "    }\n",
    "\n",
    "    # Create data directory\n",
    "    data_dir = Path('./crime_data') if data_dir is None else data_dir\n",
    "    data_dir.mkdir(exist_ok=True)\n",
    "    shapefile_dir = data_dir / 'shapefiles'\n",
    "    shapefile_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Define file paths\n",
    "    paths = {\n",
    "        'recent_crime': str(data_dir / 'recent_crime.csv'),\n",
    "        'historical_crime': str(data_dir / 'historical_crime.csv')\n",
    "    }\n",
    "\n",
    "    # Check if files need to be downloaded\n",
    "    files_exist = all([\n",
    "        Path(paths['recent_crime']).exists(),\n",
    "        Path(paths['historical_crime']).exists(),\n",
    "        (shapefile_dir / 'statistical-gis-boundaries-london').exists()\n",
    "    ])\n",
    "\n",
    "    if not files_exist or force_download:\n",
    "        print(\"Downloading data files...\")\n",
    "        # Download CSV files\n",
    "        for name in ['recent_crime', 'historical_crime']:\n",
    "            print(f\"Downloading {name}...\")\n",
    "            pd.read_csv(urls[name]).to_csv(paths[name], index=False)\n",
    "\n",
    "        # Download and extract shapefile\n",
    "        print(\"Downloading and extracting shapefile...\")\n",
    "        try:\n",
    "            r = requests.get(urls['shapefile'])\n",
    "            r.raise_for_status()\n",
    "            z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "            z.extractall(shapefile_dir)\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading shapefile: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"Data files already exist.\")\n",
    "    return paths\n",
    "\n",
    "def preprocess_data(historical_df, recent_df):\n",
    "    # Define non-date columns\n",
    "    non_date_columns = ['LSOA Code', 'LSOA Name', 'Borough', 'Major Category', 'Minor Category']\n",
    "\n",
    "    # Get date columns for each dataframe\n",
    "    historical_date_cols = [col for col in historical_df.columns if col not in non_date_columns]\n",
    "    recent_date_cols = [col for col in recent_df.columns if col not in non_date_columns]\n",
    "\n",
    "    # Create melted dataframes with 'date' column\n",
    "    historical_melted = pd.melt(\n",
    "        historical_df,\n",
    "        id_vars=non_date_columns,\n",
    "        value_vars=historical_date_cols,\n",
    "        var_name='date',\n",
    "        value_name='count'\n",
    "    )\n",
    "\n",
    "    recent_melted = pd.melt(\n",
    "        recent_df,\n",
    "        id_vars=non_date_columns,\n",
    "        value_vars=recent_date_cols,\n",
    "        var_name='date',\n",
    "        value_name='count'\n",
    "    )\n",
    "    # Combine datasets\n",
    "    combined_df = pd.concat([historical_melted, recent_melted])\n",
    "\n",
    "    # Convert date strings to datetime (add day 01 to make it a valid date)\n",
    "    combined_df['date'] = pd.to_datetime(combined_df['date'] + '01', format='%Y%m%d')\n",
    "\n",
    "    # Check for and handle duplicates\n",
    "    duplicate_check = combined_df.duplicated(subset=['LSOA Code', 'Major Category', 'Minor Category', 'date'], keep=False)\n",
    "    if duplicate_check.any():\n",
    "        print(f\"Found {duplicate_check.sum()} duplicate entries. Keeping most recent data.\")\n",
    "        combined_df = combined_df.drop_duplicates(\n",
    "            subset=['LSOA Code', 'Major Category', 'Minor Category', 'date'],\n",
    "            keep='last'\n",
    "        )\n",
    "\n",
    "    # Sort by date and other identifiers\n",
    "    combined_df = combined_df.sort_values(['date', 'LSOA Code', 'Major Category', 'Minor Category'])\n",
    "\n",
    "    # Add temporal features\n",
    "    combined_df['month'] = combined_df['date'].dt.month\n",
    "    combined_df['year'] = combined_df['date'].dt.year\n",
    "    combined_df['day_of_week'] = combined_df['date'].dt.dayofweek\n",
    "\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bcfe4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data files already exist.\n",
      "Filtered dataset shape: (8703900, 10)\n",
      "Categories: ['THEFT' 'VEHICLE OFFENCES' 'VIOLENCE AGAINST THE PERSON']\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "data_paths = download_data()\n",
    "\n",
    "recent_crime_df = pd.read_csv(data_paths['recent_crime'])\n",
    "historical_crime_df = pd.read_csv(data_paths['historical_crime'])\n",
    "\n",
    "crime_df = preprocess_data(historical_crime_df, recent_crime_df)\n",
    "\n",
    "# Filter for demo categories\n",
    "demo_categories = ['THEFT', 'VIOLENCE AGAINST THE PERSON', 'VEHICLE OFFENCES']\n",
    "crime_df = crime_df[crime_df['Major Category'].isin(demo_categories)]\n",
    "\n",
    "print(f\"Filtered dataset shape: {crime_df.shape}\")\n",
    "print(f\"Categories: {crime_df['Major Category'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69c0b102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_data(df, category):\n",
    "    cat_df = df[df['Major Category'] == category].copy()\n",
    "    # Group by LSOA and Date, summing counts (aggregating minor categories)\n",
    "    cat_df = cat_df.groupby(['date', 'LSOA Code', 'month', 'year'])['count'].sum().reset_index()\n",
    "    return cat_df\n",
    "\n",
    "def prepare_tabular_data(data, window_size=3):\n",
    "    # Create lag features\n",
    "    df = data.copy()\n",
    "    for i in range(1, window_size + 1):\n",
    "        df[f'lag_{i}'] = df.groupby('LSOA Code')['count'].shift(i)\n",
    "    \n",
    "    df = df.dropna()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d5ba83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running XGBoost for THEFT...\n",
      "XGBoost Results for THEFT: MAE=2.2457, RMSE=12.2009, WMAPE=12.2564%, rRMSE=2.6636\n",
      "Running XGBoost for VIOLENCE AGAINST THE PERSON...\n",
      "XGBoost Results for VIOLENCE AGAINST THE PERSON: MAE=1.9546, RMSE=2.6734, WMAPE=12.4798%, rRMSE=0.6828\n",
      "Running XGBoost for VEHICLE OFFENCES...\n",
      "XGBoost Results for VEHICLE OFFENCES: MAE=1.1679, RMSE=1.6435, WMAPE=17.4471%, rRMSE=0.9820\n",
      "\n",
      "Summary of XGBoost Results:\n",
      "                                  MAE       RMSE      WMAPE     rRMSE\n",
      "THEFT                        2.245677  12.200862  12.256428  2.663589\n",
      "VIOLENCE AGAINST THE PERSON  1.954618   2.673394  12.479815  0.682762\n",
      "VEHICLE OFFENCES             1.167941   1.643459  17.447124  0.982023\n"
     ]
    }
   ],
   "source": [
    "def run_xgboost(category, window_size=3):\n",
    "    print(f\"Running XGBoost for {category}...\")\n",
    "    data = get_category_data(crime_df, category)\n",
    "    df = prepare_tabular_data(data, window_size)\n",
    "    \n",
    "    dates = sorted(df['date'].unique())\n",
    "    split_idx = int(len(dates) * 0.8)\n",
    "    train_dates = dates[:split_idx]\n",
    "    test_dates = dates[split_idx:]\n",
    "    \n",
    "    train_df = df[df['date'].isin(train_dates)]\n",
    "    test_df = df[df['date'].isin(test_dates)]\n",
    "    \n",
    "    features = [f'lag_{i}' for i in range(1, window_size + 1)] + ['month', 'year']\n",
    "    target = 'count'\n",
    "    \n",
    "    X_train = train_df[features].values\n",
    "    y_train = train_df[target].values\n",
    "    X_test = test_df[features].values\n",
    "    y_test = test_df[target].values\n",
    "    \n",
    "    model = XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    preds = model.predict(X_test)\n",
    "    # Ensure non-negative predictions\n",
    "    preds = np.maximum(preds, 0)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "    \n",
    "    # Add MAPE and rRMSE\n",
    "    y_test_flat = y_test.flatten()\n",
    "    preds_flat = preds.flatten()\n",
    "    # Use WMAPE (Weighted MAPE) instead of standard MAPE to handle zeros\n",
    "    mape = (np.sum(np.abs(y_test_flat - preds_flat)) / (np.sum(y_test_flat) + 1e-8)) * 100\n",
    "    rrmse = rmse / (np.mean(y_test_flat) + 1e-8)\n",
    "    \n",
    "    print(f\"XGBoost Results for {category}: MAE={mae:.4f}, RMSE={rmse:.4f}, WMAPE={mape:.4f}%, rRMSE={rrmse:.4f}\")\n",
    "    return model, mae, rmse, mape, rrmse\n",
    "\n",
    "xgboost_results = {}\n",
    "for cat in demo_categories:\n",
    "    _, mae, rmse, mape, rrmse = run_xgboost(cat)\n",
    "    xgboost_results[cat] = {'MAE': mae, 'RMSE': rmse, 'WMAPE': mape, 'rRMSE': rrmse}\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(xgboost_results).T\n",
    "print(\"\\nSummary of XGBoost Results:\")\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
